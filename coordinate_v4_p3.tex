\documentclass[12pt,letter]{article}
\usepackage[DIV=14,BCOR=2mm,headinclude=true,footinclude=false]{typearea}
\renewcommand{\baselinestretch}{1.5} 
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{titlesec}
%\titleformat{\section}[block]{\color{blue}\Large\bfseries\filcenter}{}{1em}{}
%\titleformat{\subsection}[hang]{\bfseries}{}{1em}{}

%\usepackage{sectsty}
%\sectionfont{\centering}

\usepackage{pdflscape}

%\usepackage{MinionPro}
\usepackage{amssymb}

\usepackage{hyperref}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{color, colortbl}
\usepackage{appendix}
\usepackage{amsmath,amsthm}

%\usepackage[]{figcaps}

\usepackage{wasysym}


\newcommand{\Rho}{\mathbf{R}}
\newcommand{\Kappa}{\mathbf{C}}
\newcommand{\Omicron}{\mathbf{O}}
\newcommand\omicron{o}
\newcommand{\Tau}{\mathbf{T}}

\usetikzlibrary{arrows,shapes}

\definecolor{Gray}{gray}{0.9}

\newtheorem{result}{Result}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{lemma*}{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]


\theoremstyle{definition}
\newtheorem{example}{Example}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\theoremstyle{claim}
\newtheorem{claim}{Claim}

%\usepackage{chngcntr}
%\counterwithin{figure}{section}


\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\tikzstyle{vertex}=[circle, draw
%,fill=black!25
,minimum size=12pt
,inner sep=0pt
]
\usetikzlibrary{decorations.pathreplacing}
\tikzstyle{selected vertex} = [vertex, fill=red!24]
%\tikzstyle{unknown vertex} = [vertex, fill=black!25]
\tikzstyle{unknown vertex} = [vertex, fill=white]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]
\tikzstyle{selected edge} = [draw,line width=5pt,-,red!50]
\tikzstyle{ignored edge} = [draw,line width=5pt,-,black!20]
\newcommand{\bigtimes}{\mathbin{\tikz [x=1.4ex,y=1.4ex,line width=.2ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}%


%\linespread{1.5}

\begin{document}
%\fontsize{12}{20pt}\selectfont

\title {Coordination in Social Networks: Communication by Actions}
\author {Chun-Ting Chen}
\date{\textit{draft: v4.0}}
\maketitle

\begin{abstract}

%This paper studies a collective action problem in a setting of discounted repeated coordination games in which players know their neighbors'  inclination to participate as well as monitor their neighbors' past actions. I define \textit{strong connectedness} to characterize those states in which, for every two players who incline to participate, there is a path consisting of players with the same inclination to connect them.  Given that the networks are fixed, finite, connected, commonly known, undirected and without cycles, I show that if the priors have full support on the strong connectedness states, there is a (weak) sequential equilibrium in which the ex-post efficient outcome repeats after a finite time $T$ in the path when discount factor is sufficiently high. This equilibrium is constructive and does not depend on public or private signals other than players' actions.




\end{abstract}


\section{Introduction} 

%This paper studies collective actions in a setting of discounted repeated coordination games, where information and monitoring structures are modeled as networks. Players are uncertain about the states of nature but can observe their neighbors' actions. I would like to explore what kinds of networks can induce players to solve the underlying uncertainty in order to coordinate with the ex-post efficient outcome. Though the motive of this study is to understand the dynamic of social movements, a general interest centers on the collective action behaviors within social structures.
%
%Consider pro-democracy movements. Strong discontents overthrowing a regime may exist, but it is difficult to organized around these discontents because information about the existence of such discontents is not always transparent. For instance, in East Germany, the government had control over the electoral system and the mass media, and the eavesdropping by secret agents had impeded people from showing their discontents. As \citep{Karl-Dieter1993} or \citep{Chwe2000} have suggested, such discontents may be revealed only to someone whom you trust or have an intimate relationship with, but are hardly revealed publicly. This lack of common knowledge about the existence of strong discontent may impede people from conducting a one-shot uprising due to the fear of possible failure (e.g., \citep{Chwe2000} in proposing a static model to characterize the networks that provide common knowledge about peoples' discontents). However, an event may trigger a later event (e.g., \citep{Lohmann2011} in using informational cascade model to explain consecutive demonstrations in East Germany 1989-1991). When rebels are aware of the scale to transmit relevant information about the level of collective discontent through their actions, they might be willing to act although it might be at risk of facing failure. I view such risky actions as a part of an equilibrium strategy and the entire movement as a learning process. 
%
%
%
%
%Inspired by \citep{Chwe2000}, I model such dynamic collective action in the following way. Players repeatedly play a $k$-\textit{Threshold game} with a parameter $k$ in a network. There are two types of players located in the network, one we called them \textit{Rebel} and one we called them \textit{Inert}.  Players' types and their actions can be observed only by their neighbors. A Rebel has two actions, which are \textbf{revolt} or \textbf{stay} while an Inert has only one action, which is \textbf{stay}. A Rebel will get payoff as $1$ if he chooses \textbf{revolt} and more than $k$ players choose \textbf{revolt}; he will get payoff as $-1$ if he chooses \textbf{revolt} and less than $k$ players choose \textbf{revolt}; he will get payoff as $0$ if he chooses \textbf{stay}. An Inert will get payoff as $1$ if he chooses \textbf{stay}.
%
%Since a Rebel may not know how many Rebels exist in this world, Rebels' payoff structure captures the idea that \textbf{stay} is a safe arm and \textbf{revolt} is a risky arm. Given a common prior $\pi$ over players' types, players play this $k$-Threshold game infinitely repeatedly with a common discount factor $\delta$. Cheap talk is not allowed, no outside mechanism serves as an information sharing device. 
%
%Rebels then communicate with each other by playing actions. For different $k$ and different network structures, I am looking for a sequential equilibrium which has the property of \textit{approaching ex-post efficient} (\textit{APEX} henceforth) to investigate the information sharing behavior in the networks. An equilibrium is APEX if and only if {the tails of actions in the equilibrium path repeats the static ex-post efficient outcome after a finite time $T$}.  This refinement serves to check if players have already learnt the relevant information in the equilibrium path. If there are at least $k$ Rebels in this society, then \textit{all} Rebels should \textbf{revolt} after $T$ as if they have known that more than $k$ Rebels exist; otherwise, \textit{all} Rebels should \textbf{stay} after $T$. The Rebels' incentives to communicate are affected by their positions in networks since networks are structuring the information and monitoring structure.
%
%In order to get a quick intuition about Rebel's learning process in the proposed framework, consider the $k$-Threshold game with $k=n$ and assume that payoff is hidden. When $k=n$, a Rebel can get positive payoff only if all the players are Rebels. Given that the networks are fixed, finite, connected, commonly known, and undirected (\textit{networks} henceforth), an APEX sequential equilibrium can be constructed by a contagion-like argument. This argument is to treat \textbf{stay} as the message of ``there is an Inert out there''; and treat \textbf{revolt} as the message of ``there could be no Inert out there ''. If a Rebel has an Inert neighbor, then he plays \textbf{stay} forever. If he has no Inert neighbors, then he plays \textbf{revolt} until he observes that some of his neighbors play \textbf{stay}, and then he shifts to play \textbf{stay} forever. Since the networks are finite,  within finite periods, a Rebel will learn that there is an Inert out there if some neighbors have played \textbf{stay} and learn that there is no an Inert out there otherwise.
%
%The non-trivial cases appear when $k<n$. The $k=n$ case is easier because the underlying relevant information is to tell ``Is there an Inert out there?''. I can construct equilibrium when $k=n$ by using single-period binary actions, $\{\textbf{stay},\textbf{revolt}\}$, to separate the states into two parts, ``no Inerts'' or ``some Inerts''. In other words, these single-period actions can generate distinguishable distribution of signals to inform players in telling the true states of nature.\footnote{e.g., \citep{Fudenberg2010} or \citep{Fudenberg2011}.} 
%However, when $k<n$, the relevant information is to tell ``Are there at least $k$ Rebels out there?'', and thus these binary actions have to carry more information to reveal the states. As I will show later, several sequences of actions will be used to transmit Rebels' private informations and to control Rebels' beliefs in equilibrium. In the equilibrium path, two kinds of sequence will be used. The first kind, \textit{reporting messages}, is to report their private information about the states of nature; the second one, \textit{coordination messages}, is to inform Rebels about whether some other Rebels have known the relevant information.  Specifically, in the equilibrium path, Rebels will play the coordination message to inform other Rebels whenever they have known the relevant information, and those other Rebels will play the same message again to inform other Rebels. The coordination message means to serve as a short-cut to track individuals' higher-order beliefs about ``Have some Rebels known the relevant information?'', ``Have some Rebels known some Rebels have known the relevant information?'', etc.
%
%
%Note that communication is costly in the sense that playing \textbf{revolt} is risky. Due to being discounting, Rebels always seek the opportunity to manipulate their messages to save their costs in the time horizontal line.\footnote{Indeed, allowing cheap talk or using limit-of-mean preference (e.g., \citep{Renault1998}) will solve this coordination problem.}
%A free-rider problem may occur when reporting information incurs costs. I give an example here to illustrate this issue. Consider a situation where two nearby Rebels share information.\footnote{C.f.~Example~\ref{ex_free_rider_tree}} 
%Suppose that these two Rebels can learn the true state after acquiring information from each other's truthful reporting. Furthermore, we suppose that each of them can freely initiate the coordination after exchanging information. In this instance, truthful reporting is not the best response because a player can wait given that the other will report truthfully. The intuition behind the above scenario is to see the future coordination as a public good. This public good can only be made by Rebels' truthful reporting, which incurs some costs.
%
%
%The main result will show that this coordination problem can be solved in the {acyclic} networks . Here, I define a \textit{path} in $G$ is a sequence consisting of nodes without repetition in which a node is a neighbor of a previous node. Then I define an undirected network acyclic $G$ by defining a network in which the path between different nodes is unique. After I define \textit{strong connectedness} as the property that there is always a path consisting of Rebels to connect any pairs of Rebels,  the main result shows:
%
%\begin{result}\textbf{(Main Result)}
%For $n$-person repeated $k$-Threshold game with parameter $1\leq k \leq n$ played in any acyclic network, if $\pi$ has full support on the strong connectedness, then, there is a $\delta^{*}$ such that a (weak) APEX sequential equilibrium exists whenever $\delta>\delta^{*}$.  
%\end{result}
%
%Here, the assumption that $\pi$ has full support on strong connectedness means that $\pi$ assigns positive probability on same states if and only if those states exhibit strong connectedness. This assumption is to make sure that the underlying game is not reduced to an incomplete information game which is without communication.  To see this, recall that an Inert always plays \textbf{stay}. Rebels cannot communicate with some Rebels by their actions if an Inert happens to separate them. For instance, in a wheel network, an incomplete game without communication is that the central player is an Inert while the peripheral players are all Rebels. It is impossible to find an APEX equilibrium in this instance unless $k=1$.
%
%The out-of-path belief serves as a grim trigger as follows. Whenever a Rebel detects a deviation, he believes that all other players outside his neighborhood are Inerts. Thus, if there are less than $k$ Rebels in his neighborhood, he will play \textbf{stay} forever. With this out-of-path belief and the constructed equilibrium strategies, the belief system satisfies \textit{updating consistency}(\citep{Perea2002}), while it may not satisfy full consistency (\citep{Krep_Wilson1982}). \footnote{Updating consistency requires that, for every player, for every player's strategies, for every information sets $s^1,s^2$ where $s^2$ follows $s^1$, if $s^2$ happens with positive probability given $s^1$ and given players' strategies contingent on $s^1$, then the belief over $s^2$ should satisfy Bayesian updating conditional on the belief over $s^1$ and players' strategies contingent on $s^1$. In other words, the updating consistency require that players hold beliefs in every information sets and hold updated beliefs that follows previous beliefs. This requirement imposes restrictions on out-of-path beliefs that induce sequential rationality, although it is weaker than full consistency in the sense that full consistency implies updating consistency.}
%
%The establishment of an equilibrium construction starts from building a communication protocol. By exploiting the assumption of finite and commonly known network, I assign each node a distinct prime number. Then I let reporting messages carry the information about the multiplication of nodes' prime numbers. Since the multiplication of prime numbers can be defactorized uniquely, the reporting messages thus carry the information about those nodes' locations in a network. Next, I let two phases, reporting period and coordination period, occur in turns in the time horizon, where the reporting (resp. coordination) messages are played in the reporting (resp. coordination) period. In the coordination period, whenever a Rebel tells the relevant information, such Rebel inform his nearby Rebels by sending coordination messages. Those nearby Rebels then continue to inform their nearby Rebels by sending coordination messages, etc. Then, after the coordination period, if a Rebel has received a coordination message, he is certain that all Rebels have commonly known that they can tell the relevant information. 
%
%I call a complete two-phases, starting from a reporting period and ending with a following coordination period, a \textit{block}.  In a block, I control the inter-temporal incentives in playing between reporting and coordination messages as follows. First, I let both of the coordination messages, one of them can initiate the coordination to \textbf{revolt} and another one can initiate the coordination to \textbf{stay}, incur no expected cost. Second, I let Rebels play \textbf{revolt} after a block only if they have observed the coordination message to \textbf{revolt} and observed some reporting messages  which incur some expected costs. However, the continuation behavior after observing the coordination message to \textbf{stay} is not contingent on any reporting message. When a Rebel looks forward future coordination to \textbf{revolt}, he may have the incentive to take a risk to influence Rebels' future behavior forwardly; otherwise, he just plays \textbf{stay}. Next, in the equilibrium path, I make sure that Rebels will play ex-post efficient outcome repeatedly right after a block if some Rebels have initiated the coordination in that block. I will argue that only those Rebels who have been able to tell the relevant information after reporting period have the incentive to initiate the coordination. This is because they do not need further evidence to prove that whether the revolution will be successful. This argument is to show that a Rebel other than them will not take advantage to send that free coordination message to initiate the coordination. This is because players cannot update further information if all their neighbors continue to play the same actions in the future. When $\delta$ is high enough, he will not initiate the coordination to impede his own learning process to achieve the ex-post efficient outcome.
%
%I then characterize Rebels' incentive in taking risks and control how much \textbf{revolt} they should play to sustain an APEX equilibrium. In the equilibrium path, a Rebel iteratively updates his relevant information given other Rebels' \textbf{revolt}-\textbf{stay} finite sequence in reporting their information about the state, and a Rebel takes risks only if his current relevant information has not been acquired by other Rebels. In the equilibrium path, a Rebel thus believes that ``more other Rebels are out there'' if and only if his nearby Rebels take risks to report their existence. Some specified forms of reporting messages are introduced, and the out-of-path belief is to enforce Rebels not to play differently from them.
%
%The key step here is to construct a special reporting message which incurs the least expected cost in taking risks, and this message should be considered as a part of the equilibrium path. I denote this special reporting message as $\langle 1 \rangle$. To see its importance, consider the concept of pivotal Rebel. Here, a pivotal Rebel is defined as the Rebel who is sure that he can know the relevant information right after a reporting period given that other Rebels will report their information truthfully. Now suppose playing $\langle 1 \rangle$ is not considered as a part of the equilibrium path, and suppose a Rebel finds himself as a pivotal Rebel during a reporting period while he has not yet reported anything in that period. It could be possible for him to find a profitable deviation by taking less risks (i.e. by playing less \textbf{revolt}), which can not be detected by at least $k$ Rebels although some Rebels can indeed detect such deviation. Since those Rebels who detected such deviation will play \textbf{stay} forever by the out-of-path belief, and this pivotal Rebel can initiate the coordination to \textbf{revolt} by convincing other Rebels to play \textbf{revolt}, the APEX fails. To solve this problem, I introduce message $\langle 1 \rangle$ to let pivotal Rebels identify themselves, while I let coordination messages to \textbf{revolt} or to \textbf{stay} have to be initiated when $\langle 1 \rangle$ has been played in the equilibrium path to prevent non-pivotal Rebels from mimicking pivotal Rebels.
%
%The major difficulties remaining to solved are the situations where there are multiple pivotal players nearby each other. In such phenomenon, the APEX may fail since playing $\langle 1 \rangle$ does not answer ``how many Rebels has a pivotal Rebel known?'' although it does address ``a pivotal Rebel exists''. The assumption of acyclic networks is crucial of solving these problems. If the networks are acyclic, I will show it later that there are only two kinds of pivotal Rebels. One kind is that they have known there are at least $k-1$ Rebels. The other kind is that they will know the true state given other Rebel neighbors' truthful reporting. I call the latter case a free-rider problem. If the networks are acyclic, Lemma~\ref{lemma_at_most_two_nodes} will show that the free-rider problems only happen between two nearby pivotal Rebels in only one block in the equilibrium path. Further, these two nearby Rebels will know that this free-rider problem will occur before the game entering into this block. The consequence of Lemma~\ref{lemma_at_most_two_nodes} is that, before the game entering into this block, I can let arbitrary one of them report the information about the state and let the other one play $\langle 1 \rangle$ dependent on their indexed prime numbers.
%\footnote{
%Such scenario substantially differs from the cyclic counterpart. The free-rider problem becomes intractable in cyclic networks. Let's consider the following example, in which there are 5 players in a cyclic network. Player $i$ is a Rebel and labeled as \textit{RB} if $i\in\{1,2,3,4,\}$; is an Inert if $i\in\{5\}$.
%
%\begin{center}
%\begin{tikzpicture}[scale=1]
%    % Draw a 7,11 network
%    % First we draw the vertices
%    \foreach \pos/\name in {{(2,3)/RB_1}, {(3,2)/RB_2}, {(5,2)/RB_3}, {(6,3)/RB_4}, {(4,5)/5}}
%        \node[vertex] (\name) at \pos {$\name$};
%    
%    
%    % Connect vertices with edges 
%    \foreach \source/ \dest in { RB_1/RB_2, RB_2/RB_3, RB_3/RB_4, RB_4/5, 5/RB_1}
%        \path[edge] (\source) -- (\dest) ;
%        
%\end{tikzpicture}
%\end{center}
%
%Let's suppose $k=4$ and assume that, at a certain period $t$, players know only their immediate neighbors' types. Let's further simplify the scenario and suppose that any Rebel player can share information with his neighbors by writing that incurs a fixed cost $c$ at period $t+1$, as well as freely initiate coordination at $t+2$. In such a scenario, for \textit{any} set of players, the answer to ``Who are the pivotal players before entering $t+1$?'' is not certain at $t$. Take the set of players $\{1,2\}$ as an example, and notice that, from the perspective of player 2, the type of player 5 could be Inert. Therefore, player 2 does not know whether player 1 is a pivotal player though player 2 himself is one. (In this case, player 1 is indeed not a pivotal player.) Similarly, player 2 does not know whether player 3 is a pivotal player even player 3 is indeed a pivotal player. The arbitrary selection of free riders, say by choosing player 1, might then fail to reach coordination at $t+2$. 
%
%However, if we can cut the edge between player 4 and player 5 such that the network becomes a tree, player 2 knows that he is the only pivotal player.
%}

%
%
%
%
%This paper contributes to several fields of economics. 
%
%First, the future coordination can be viewed as a public good among all Rebels. A strand of public good literature, such as \citep{Lohmann1994}, is to view information as a public good while generating information is costly.\footnote{For instance, \citep{Lohmann1993}\citep{Lohmann1994} consider that individuals generate information by their actions, where the aggregate outcomes of actions is public. \citep{Bolto_Harris1999} consider team experiment in infinite time horizon where the outcomes of experiments are public signals. \citep{Bramoulle2007} view information as a public good and consider public good provision in networks.}
%This paper models costly information generation, while adding another aspect, network-monitoring, to investigate a collective action behavior.
%
%Second, this paper is also related to the literature of social learning.\footnote{Reviews can be seen in \citep{Bikhchandani1998} \citep{Cao2001}.}
%Several papers have considered social learning in networks.\footnote{\citep{Goyal2012} gives the reviews. Recent papers, e.g., \citep{Acemoglu2011} and \citep{Chatterjee2011} discuss this topic}
%In this literature, when players are myopic, the information flows could be very complicated because the information they sent can in turns affect their future behaviors. For instance, in \citep{RePEc:eee:gamebe:v:45:y:2003:i:2:p:329-346},  even for 3-person connected undirected networks, the complete network and incomplete network will give different convergence results which highly depend on individuals' initial private signals and their allocations in a network. In \citep{Golub2010}, instead of using Bayesian learning, they use a naive learning protocol to tackle this social learning problem. I consider the social learning in networks as a learning-in-game procedure, where individuals can put more weights on the future learning results. My result gives a hint that the shape of network (without cycle) did not matter too much if players are far-sighted.
%
%Third, a growing literature considers the game played in networks where various games played in various networks with various definitions.\footnote{\citep{Jackson2008}\citep{Goyal2012} gives the reviews.}
%Only avfew papers in this literature discuss the repeated game. In complete information game. In \citep{Laclau2012}, she proves a folk theorem where players play the game locally. In \citep{Wolitzky2013} \citep{Wolitzky2014}, he considers network-like monitoring where a prisoner dilemma game is played globally. My paper is the first paper to consider the incomplete information game repeatedly played in a network. 
%
%My paper is also related to the literature in folk theorems in discounted repeated games with incomplete information. In this literature, they consider more general games than the games adopted here. \citep{Fudenberg2010} \citep{Fudenberg2011} \citep{Wiseman2012} considering $n$-person game with public signals jointly generated by the states and actions; \citep{Yamamoto2014} considering $2$-person game with private signals jointly generated by the states and actions. There, the full-rank conditions are imposed to let single-period actions generate informative signals to separate the states. Here, I consider $n$-person game without signals and thus the single-period full-rank conditions are not imposed before solving the equilibrium.  And my result shows that acyclic networks are sufficient to sustain the ex-post efficiency when discount factor is sufficiently high. 
%
%
%
%The paper is organized as the followings. I introduce the model in Section~\ref{sec:model} . I discusses the equilibrium construction and shows the main result in Section~\ref{sec:equilibrium_1} and Section~\ref{sec:equilibrium_2} respectively. Some variations of my model will be discussed in its subsection~\ref{sec:varies}. The conclusion is made within Section~\ref{sec:con}. All the missing proofs can be found in Appendix~\ref{appx_network}.

\section{Model}
\label{sec:model}
%\subsection{Notations}


There is a set of players $N=\{1,...,n\}$. They constitute a network $G=(V,E)$ so that the vertices are players ($V=N$) and an edge is a pair of them ($E$ is a subset of the set containing all two-element subsets of $N$). Throughout this paper, $G$ is assumed to be finite, commonly known, fixed, undirected, and connected.\footnote{A path in $G$ from $i$ to $j$ is a finite sequence $(l_1,l_2,...,l_L)$ without repetition so that $l_1=i$, $l_L=j$, and $\{l_q,l_{q+1}\}\in E$ for all $1\leq q<L$. $G$ is fixed if $G$ is not random, and $G$ is undirected if there is no order relation over each edge. $G$ is connected if, for all $i,j\in N$, $i\neq j$, there is a path from $i$ to $j$.}

Time is discrete and denoted by $S=\{0,1,...\}$ with index $s$. Each player could be either type $R$ or type $I$ assigned by the nature at $s=0$ according to a common prior $\pi$; $R$ or $I$ represents a Rebel or an Inert respectively. Call $\theta\in \Theta:= \{R,I\}^n$ a state of nature. At each $s\geq 1$, players play a normal form game, the \textit{$k$-threshold game}, infinitely repeated played with common discounted factor $\delta\in (0,1)$. In the $k$-threshold game, $A_R=\{\textbf{revolt}, \textbf{stay}\}$ is the set of actions for $R$ and $A_I=\{\textbf{stay}\}$ is that for $I$. Denote by $\#X$ the cardinality of an set $X$. A Rebel $i$'s stage-game payoff function is defined as below, while an Inert's stage-game payoff is equal to $1$ no matter how other players play. 
\[   
\begin{array}{ll}
      u_{R}(a_{i},a_{-i})=1 & \text{if $a_{i}=\textbf{revolt}$ and $\#\{j:a_{j}=\textbf{revolt}\}\geq k$} \\
      u_{R}(a_{i},a_{-i})=-1 & \text{if $a_{i}=\textbf{revolt}$ and $\#\{j:a_{j}=\textbf{revolt}\}< k$} \\
      u_{R}(a_{i},a_{-i})=0 & \text{if $a_{i}=\textbf{stay}$} \\
\end{array} 
. \]



Let $[R](\theta)$ be the set of Rebels given $\theta$ and the notion \textit{relevant information} indicate whether or not $\#[R](\theta)\geq k$. Note that the ex-post efficient outcome in the stage game is that every Rebel plays \textbf{revolt} whenever $\#[R](\theta)\geq k$, and plays \textbf{stay} otherwise.\footnote{Moreover, at every $\theta$ and every $k$, the ex-post efficient outcome is unique and gives the maximum as well as the same payoff to every Rebel.} 

During the game, every player can observe his and his neighbors' types and his and their histories of actions, but no more. A history of actions played by $i$ from period one to period $s\geq 1$ is denoted by $h^s_i\in H^s_{i}:=\bigtimes^s_{\varsigma =1} A_{\theta_i}$. Let $G_i:= \{j:\{i,j\}\in E\}$ be $i$'s \textit{neighbors}. Denote $\theta_{G_i}\in \Theta_{G_i}:= \{R,I\}^{G_i}$ as the type profile of $i$'s neighbors. Let $h^0_i=\emptyset$, and denote $h^s_{G_i}\in H^s_{G_i}:=\bigtimes_{j\in G_i}\bigtimes^s_{\varsigma =1}H^{\varsigma}_j$ as a history of actions played by $i$'s neighbors from period one to period $s\geq 1$. The information set of $i$ about $\theta$ at every period is the cylinder $p(\theta)=\{\theta_{G_i}\}\times \{R,I\}^{N\setminus G_i}$, and the information set about histories of action from period one to period $s\geq 1$ is $\{h^s_{G_i}\}\times H^s_{N\setminus G_i}$. A player $i$'s pure behavior strategy $\tau_{i}$ is a measurable function with respect to $i$'s information partition if $\tau_i$ maps $\{\theta_{G_i}\}\times \{R,I\}^{N\setminus G_i}\times \{h^s_{G_i}\}\times H^s_{N\setminus G_i}$ to a single action in his action set for every $s\in\{1,2,...\}$ and every $\theta\in \Theta$. I assume that payoffs are hidden to emphasize that observing neighbors' actions are the only channel to infer other players' types and actions.\footnote{Such restriction will be relaxed in the Section~\ref{sec:varies}.} 

Likewise, define $H^s:=\bigtimes_{j\in N} H^s_{j}$ as the set of histories of actions from period one to period $s\geq 1$ and $H:= \bigcup^{\infty}_{\varsigma=0} H^{\varsigma}$ as the collection of histories of actions. By abusing the notation a bit, let $h({\tau},\theta)\in H$ denote the realized history of actions generated by strategy profile $\tau=(\tau_1,\tau_2,...,\tau_n)$ given $\theta$. Designate $\alpha^{\pi,\tau}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i})$ as the conditional distribution over $\Theta\times H^s$ induced by $\pi$ and $\tau$ conditional on $i$'s information up to period $s\geq 1$. The belief of $i$ over $\Theta$ induced by $\pi$ and $\tau$ up to period $s\geq 1$ is defined by 
\[\beta^{\pi,\tau}_{G_i}(\theta|h^{s}_{G_i}):= \sum_{h^{s}\in H^s}\alpha^{\pi,\tau}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i}).\]

The equilibrium concept is the weak sequential equilibrium.\footnote{A weak sequential equilibrium is an assessment $\{\tau^{*}, \alpha^{*}\}$, where $\alpha^{*}$ is a collection of distributions over players' information sets with the property that, for all $i\in N$ and for all $s=1,2,...$, $\alpha^{*}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i})=\alpha^{\pi,\tau^{*}}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i})$ whenever the information set is reached with positive probability given $\tau^{*}$. Moreover, for all $i\in N$ and for all $s=1,2,...$, $\tau^{*}_{i}$ maximizes $i$'s continuation expected payoff of
\[E^{\delta}_G(u_{\theta_i}(\tau_{i},\tau^{*}_{-i})|\alpha^{\pi,\tau_{i},\tau^{*}_{-i}}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i}))\] conditional on $\theta_{G_i}$ and $h^{s}_{G_i}$ for all $h^{s}_{G_i}\in H^s_{G_i}$.} 
My objective is looking for the existence of \textit{approaching ex-post efficient equilibrium} or \textit{APEX equilibrium}, which is defined below.

\begin{definition}[APEX strategy]
A behavior strategy $\tau$ is APEX  if for all $\theta$, there is a terminal period $T^{\theta}<\infty$ such that the actions in $h^{\tau}_{\theta}$ after $T^{\theta}$ repeats the static ex-post Pareto efficient outcome.
\end{definition}

\begin{definition}[APEX equilibrium]\label{Def_ex-post_efficient}
An equilibrium $(\tau^{*},\alpha^{*})$ is APEX if $\tau^{*}$ is APEX.
\end{definition}




In an APEX strategy, all Rebels will play \textbf{revolt} forever after some period only if $\#[R](\theta)\geq k$; otherwise, Rebels will play \textbf{stay} forever after some period. It is as if the Rebels will learn the relevant information in the equilibrium because they will play the ex-post efficient outcome after a certain point of time and keep on doing so. Notice that, in an APEX equilibrium, it is not only as if the Rebels will learn the relevant information: they must learn that. Lemma~\ref{lemma_learn} articulates this fact.

\begin{lemma}[Learning in the APEX equilibrium]\label{lemma_learn}
If the assessment $(\tau^*,\mu^{*})$ is an APEX equilibrium, then for all $\theta\in \Theta$, there is a finite time $T^{\theta}_i$ for every Rebel $i$ so that
\[\sum_{\theta\in\{\theta:[R](\theta)\geq k\}}\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})= \text{ either } 1 \text{ or } 0\]
whenever $s\geq T^{\theta}_i$.
\end{lemma}



\begin{definition}[Learning the relevant information]\label{def_learn}
A Rebel $i$ learns the relevant information at period $\varsigma$ according to strategy $\tau$ if $\sum_{\theta\in\{\theta:[R](\theta)\geq k\}}\beta^{\pi,\tau}_{G_i}(\theta|h^{s}_{G_i})=$ either $1$ or $0$ whenever $s\geq \varsigma$.
\end{definition}

It is clear that an APEX equilibrium exists when $k=1$. As for other cases, let us start with the case of $k=n$ and then continue on to the case of $1<k<n$. The proof is by construction. In the case of $k=n$, the constructed APEX equilibrium is intuitive and satisfies a stronger equilibrium concept. My main result tackles the case of $1<k<n$. In such case, my constructed APEX equilibrium is not trivial and can only works for acyclic networks. Section~\ref{sec:cyclic} discusses why my constructed equilibrium is intractable in cyclic networks.

\section{Equilibrium: APEX for $k=n$}
\label{sec:equilibrium_1}

In this section, my objective is to show the existence of APEX equilibrium for the case of $k=n$. In this case, notice that a Rebel can get a better payoff from playing \textbf{revolt} than from \textbf{stay} \textit{only if} all players are Rebels. Two consequences follow. Firstly, if a Rebel has an Inert neighbor, this Rebel will always play \textbf{revolt} in the equilibrium. Secondly, at any period $s\geq 1$, it is credible for every Rebel to punish a deviation by playing \textbf{stay} forever \textit{if} there is another one who also plays \textbf{stay} forever, independently from the belief held by the punisher. These two features constitute an APEX equilibrium and further transform itself to a sequential equilibrium. 

\begin{theorem}[APEX equilibrium for the case of $k=n$]
\label{thm_minor_thm}
For any $n$-person repeated $k$-Threshold game with parameter $k=n$ played in a network, there is a $\delta^{*}\in(0,1)$ such that a sequential APEX equilibrium exists whenever $\delta >
\delta^{*}$.
\end{theorem}


Image that there are an Inert somewhere as well a Rebel $i$ somewhere. Since the network is connected, there is a path connecting these two players. Along with this path, consider the ``closest'' Inert from Rebel $i$; this is an Inert who can be reached by the least number of consecutive edges from $i$. Note that this Inert's Rebel neighbors will play \textbf{stay} forever since $k=n$. Consider a strategy for Rebels on this path: a Rebel will play \textbf{stay} only after observing his neighbor plays \textbf{stay}.  On this path and according to this  strategy, Rebel $i$ will know the existence of such Inert eventually since the network is finite. This contagion argument suggests the following APEX strategy. Every Rebel plays \textbf{revolt} initially except for he has an Inert neighbor. Each of them will continuously play \textbf{revolt} but switch to \textbf{stay} instantly if he observes any of his neighbor plays \textbf{stay}. Upon observing a $n$ consecutive \textbf{revolt}, a Rebels knows that no Inert exists; otherwise, he knows some Inert exists. The above strategy is an APEX strategy if all Rebels play ex-post efficient outcome after peiord $n$. To extend it to be an APEX equilibrium, let the deviant play \textbf{stay} forever and the punisher who detects it also play \textbf{stay} forever. This out-of-path strategy is credible for both the deviant and the punisher, independent from the belief held by the punisher, and hence it is also sequential rational.\footnote{This sequential rationality is in the sense of \citep{Krep_Wilson1982}.}




\section{Equilibrium: APEX for $1<k<n$}
\label{sec:equilibrium_2}

I show the existence of APEX equilibrium for the case of $1<k<n$ in this section. In contrast to the case of $k=n$, a Rebel still has the incentive to play \textbf{revolt} even if he has an Inert neighbor. This opens a possibility for the non-existence of APEX equilibrium. Example~\ref{ex_strong_connectedness} below demonstrates it.

\begin{figure}

\begin{center}
%\fbox{
%\begin{minipage}{13 cm}
\begin{tikzpicture}[scale=1.5]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(1,0)/R_1}, {(2,0)/I_2}, {(3,0)/R_3}}
        \node[vertex] (\name) at \pos {$\name$};
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/I_2, I_2/R_3}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
%\end{minipage}
%}
\end{center}

\caption{The state and the network in which the APEX equilibrium does not exist when $k=2$.}
\label{fig:strong_connectedness}
\end{figure}

\begin{example}\label{ex_strong_connectedness}
Suppose $k=2$ and $\theta=(R,I,R)$. The state and the network is represented in Figure~\ref{fig:strong_connectedness}. Rebel 1 never learns the type of player 3 since Inert 2 cannot reveal it, and hence no APEX equilibrium exists in this scenario.
\end{example}

The following assumption on the prior---\textit{full support on strong connectedness}---excludes the possibility of nonexistence of APEX equilibrium. To this end, I begin with the definition of \textit{strong connectedness}.

\begin{definition}[Strong connectedness]
\label{def:strong_connectedness}
Given $G$, a state $\theta$ has strong connectedness if, for every two Rebels, there is a path consisting of Rebels to connect them.

\end{definition}  

In the language of graph theory, the following definition is equivalent: given $G$, $\theta$ has strong connectedness if the induced graph by $[R](\theta)$ is connected.

\begin{definition}[Full support on strong connectedness]\footnote{The prior has full support on strong connectedness is stronger than every state has strong connectedness is common knowledge. This marginal requirement is convenient in constructing equilibrium. The main result only requires a weaker version---$\pi(\theta)>0\Rightarrow \text{ $\theta$ has strong connectedness }$. Working on this weaker version is at the expense of much tedious proof, however.}
Given $G$, $\pi$ has full support on strong connectedness if 
\[\pi(\theta)>0\Leftrightarrow \text{ $\theta$ has strong connectedness}.\]
\end{definition}  

I state the main characterization of this paper:
\begin{theorem}[APEX equilibrium for the case of $1<k<n$]
\label{thm_main_result}
For any $n$-person repeated $k$-Threshold game with parameter $1<k<n$ played in networks, if networks are acyclic and if $\pi$ has full support on strong connectedness, then there is a $\delta^{*}\in(0,1)$ such that an APEX equilibrium exists whenever $\delta>\delta^{*}$.\footnote{A network is acyclic if the path from $i$ to $j$ for all $i\neq j$ is unique.}

\end{theorem}

Constructing an APEX equilibrium in this case is convoluted. I illustrate the proof idea throughout this paper while leaving the formal proof in Appendix. Moreover, since the case of $k=2$ is trivial given that $\theta$ has strong connectedness, I focus on $2<k<n$ cases.\footnote{Suppose $[R](\theta)\geq k=2$, by the full support on strong connectedness, each Rebel have a Rebel neighbor. The following strategy is an APEX strategy. A Rebel plays \textbf{revolt} from period one afterwards if he has a Rebel neighbor; otherwise, he plays \textbf{stay} forever from period one. It extends to an APEX equilibrium by letting the out-of-path belief be assigning probability one to the event that all non-neighbors are Inerts.} 

I start with considering a specific APEX strategy as the framework in constructing an APEX equilibrium in Section~\ref{sec:apex_strategy}; incentive compatibility is not incorporated at this moment. Then I introduce an auxiliary scenario to illuminate the discussion of incentive compatibility in Section~\ref{sec:writing}. I then go back to present essential details of the constructed APEX equilibrium in Section~\ref{sec:dis_writing}. 

\subsection{APEX strategy}
\label{sec:apex_strategy}
To begin, the following example is examined.
\begin{example}
\label{ex:leading}
Let $k=5$ and consider the configurations in Figure~\ref{fig:circle_with_bridge_5} and Figure~\ref{fig:circle_with_bridge_4}. Note that there are 6 players in both of these configurations; 5 Rebels are in Figure~\ref{fig:circle_with_bridge_5} and 4 Rebels are in Figure~\ref{fig:circle_with_bridge_4}. The below is an APEX strategy.
\[\textbf{Strategy for Example~\ref{ex:leading}:}\]
\begin{enumerate}
\item At period $j$, $1\leq j \leq 6$, each Rebel plays \textbf{revolt} if himself is $j$ or any of his Rebel neighbors is $j$; otherwise, he plays \textbf{stay}. 
For instance, from period one to period 6, what Rebel 3 plays is $(\textbf{revolt},\textbf{stay},\textbf{revolt}, \textbf{revolt},\textbf{stay}, \textbf{revolt})$ in the configuration in Figure~\ref{fig:circle_with_bridge_5}. Consequently, implied by Rebel 3's actions, in the configuration in Figure~\ref{fig:circle_with_bridge_5}, right after period 1, 2, 3, or 4, Rebel 4 is still aware the existence of Rebel 1, 2, 3, and 4; right after period 5, Rebel 4 is aware that player 5 is an Inert; right after period 6, Rebel 4 is aware that player 6 is a Rebel. Similar behavior applies to other players. 
\item Right after period 6, at period 7, by observing his neighbors' actions, a Rebel who has learnt the number of Rebels exceeding or equal to $k$ plays $(\textbf{revolt},\textbf{revolt})$ consecutively. Then he plays \textbf{revolt} from period 19 afterwards. Likewise, a Rebel who has observed the number of Rebels, which is implied by his neighbors' actions, is less than $k$, he plays $(\textbf{stay},\textbf{stay})$ consecutively. Then he plays \textbf{revolt} from period 19 afterwards. A Rebel whose observation does not belong to the above two categories plays the sequence $(\textbf{revolt},\textbf{stay})$ consecutively and plays \textbf{stay} forever after period 19. 
\item From period 7 to period $19$, a Rebel plays $(\textbf{revolt},\textbf{revolt})$ consecutively right after observing the same sequence played by any of his neighbor; he then plays \textbf{revolt} from period 19 afterwards. Correspondingly, a Rebel plays $(\textbf{stay},\textbf{stay})$ consecutively right after observing the same sequence played by any of his neighbor and then plays $\textbf{stay}$ from period 19 afterwards. 
\item The out-of-path strategy is simple: ignoring the deviation and following the in-path strategy in which more Rebels can be implied. 
\end{enumerate}

By following the in-path strategy of the above, both Rebel 1 and 4 learn the relevant information right after period 7, and all Rebels play the ex-post outcome from period 19 afterwards. This strategy is thus an APEX strategy with a straightforward idea: from period one to period 6, ``a {reporting phase}'', Rebels reports who their Rebel neighbors are by sequences of actions, and, from period 7 to period 19, ``a {coordination phase}'', they coordinate themselves to disseminate the knowledge whether the relevant information is learnt. 
\end{example}

The above strategy, nevertheless, calls for revising to seemly integrate incentive compatibility. Notice that Rebel 6's actions are redundant: the ex-post efficient outcome will be achieved anyway despite how Rebel 6 plays. This is because Rebel 3's neighbors include Rebel 6's and how Rebel 6 plays cannot be observed by other Rebels outside Rebel 3's neighborhood. The notion ``redundant'' will be replaced by and formally defined as \textit{inactive} later to emphasize that Rebel 6 can actually report nothing. What is more, Rebel 4 has no incentive to report. This is because Rebel 1 reports the identical information to the same Rebels. In other words, the ex-post efficient outcome will be played anyway despite how Rebel 4 plays. Rebel 1's position is exactly the same as Rebel 4's, and thus both of them have no incentive to report. A free-rider problem exists.\footnote{It is worth noting that whether a free-rider problem could happen is not solely determined by whether a network is cyclic. It depends on how an APEX strategy is constructed and exists in the constructed APEX equilibrium for Theorem~\ref{thm_main_result}, in which only acyclic networks are of concern. The formal definition of free-rider problem is in Section~\ref{sec:dis_writing}, and a deeper issue will be discussed in Section~\ref{sec:cyclic}.} 

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(2,1)/R_1}, {(3,2)/R_2}, {(1,2)/R_3}, {(2,3)/R_4}, {(4,2)/I_5}, {(0,2)/R_6}}
        \node[vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}}
%    \node[unknown vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%    \node[selected vertex] (\name) at \pos {$\name$};
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_4, R_1/R_2, R_1/R_3, R_2/R_4,R_3/R_4, R_2/I_5, R_3/R_6}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which players 1, 2, 3, 4, 5 are Rebels and players 6 is an Inerts.}
\label{fig:circle_with_bridge_5}
\end{figure}

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(2,1)/R_1}, {(3,2)/R_2}, {(1,2)/R_3}, {(2,3)/R_4}, {(4,2)/I_5}, {(0,2)/I_6}}
        \node[vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}}
%    \node[unknown vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%    \node[selected vertex] (\name) at \pos {$\name$};
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_4, R_1/R_2, R_1/R_3, R_2/R_4,R_3/R_4, R_2/I_5, R_3/I_6}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which players 1, 2, 3, 4 are Rebels and player 5, 6 are Inerts.}
\label{fig:circle_with_bridge_4}
\end{figure}

As follows, a revision of Strategy for Example~\ref{ex:leading} is designed and meant to be the foundation framework in constructing the APEX equilibrium for Theorem~\ref{thm_main_result}. Let $\mathbf{I}$ be all possible sets  of Rebels amongst states in the network.\footnote{To be precise, $\mathbf{I}=\{X\subseteq N: \theta_j=R \Leftrightarrow j\in X \text{ for some $\theta\in\Theta$}\}$} 
For convenience, construct a set $W$ that consists of sequences of actions, in which all sequences have equal length, so that there is a one-to-one mapping between $\mathbf{I}$ and $W$. This $W$ exists. For an example, which is used in the constructed APEX equilibrium for Theorem~\ref{thm_main_result}, let the length of each sequence in $W$ be the multiplication of a series of prime numbers. In this series, each prime number is distinct and assigned to a distinct player. Denote $x_i$ as the prime number assigned to $i$. The length of a sequence in $W$ is therefore $\bigotimes_{j\in N}x_j=x_1\otimes...\otimes x_n$, where $\otimes$ is the usual multiplication operator. If $I\in\mathbf{I}$, the corresponding $\langle I \rangle\in W$ is crafted to be:  
\[(\overbrace{\textbf{stay},...,\textbf{stay},\underbrace{\textbf{revolt},\textbf{stay},...,\textbf{stay}}_{\bigotimes_{j\in I}x_j}}^{\bigotimes_{j\in N} x_j}).\]
A one-to-one mapping between $\mathbf{I}$ and ${W}$ exists since a multiplication of prime numbers can be uniquely factorized. 

Fix $W$ and partition the time into two consecutively alternating phases: \textit{coordination phase} and \textit{reporting phase}, while the time is starting from the coordination phase. The $t$-th completion of two consecutive phases is called the $t$-block. The length of each reporting phase is equal to the length of $w\in W$, and the length of each coordination phase is twice of the total number of players. Figure~\ref{fig:ordered_original_game_intro} depicts this partition.\footnote{More precisely, partition the time by $\{\{0\},\{1,...,s_{l_1},s_{l_1}+1,...s_1\},\{s_1+1,...,s_{l_2},s_{l_2}+1,...,s_2\},...,\{s_{t-1}+1,...,s_{l_{t}},s_{l_t}+1,...,s_t\},...\}$, where $t=1,2,...$ and $s_{0}=0$, so that the length of $\{s_{l_t}+1,...,s_t\}$ is equal to the length of $w\in W$ for each $t$, while the length of $\{s_{t-1}+1,...,s_{l_t}\}$ is $2n$. Call $\{s_{t-1}+1,...,s_t\}$ the $t$-block. call $\{s_{l_t}+1,...,s_t\}$ the \textit{reporting phase} at $t$-block and call $\{s_{t-1}+1,...,s_{l_t}\}$ the \textit{coordination phase} at $t$-block.}

\begin{figure}
\[0<\underbrace{(\text{coordination phase})<(\text{reporting phase})}_{\text{$1$-block}}<\underbrace{(\text{coordination phase})<(\text{reporting phase})}_{\text{$2$-block}}<...\]
\caption{The partition of the time in the repeated $k$-threshold game. $<$ is the linear order relation over the time.}
\label{fig:ordered_original_game_intro}
\end{figure}

Given $\theta$, denote the set $I_i$ as $i$'s Rebel neighbors. If $i$ is a Rebel, let $I^1_i= I_i$, and let $I^t_i= \bigcup_{j\in G_i} I^{t-1}_j$ for $t\geq 2$. If $j$ is an Inert, let $I^t_j=\emptyset$ for $t\geq 1$. Put it differently, $I^t_i$ is the set of Rebels who can be reached from Rebel $i$ by a path consisting of Rebels and of which the length is at most $t$. Let us then identify a set of Rebels, \textit{active Rebels}, who are crucial in the information sharing process. For this purpose, first define $G^t_i$ for each $t$: if $i$ is a Rebel, let $G^1_i= G_i$, and let $G^t_i= \bigcup_{j\in G_i} G^{t-1}_j$ for $t\geq 2$; if $j$ is an Inert, let $G^t_j=\emptyset$ for $t\geq 1$. That is, $G^t_i$ is the set of players who can be reached from Rebel $i$ by a path consisting of Rebels and of which the length is at most $t$. Finally, define the active Rebels at $t$-block as follows.
\begin{definition}[Active Rebel at $t$-block]
Set $R^0=[R](\theta)$. The set of active Rebels in the $t$-block is 
\[\text{$R^t:= \{i\in R^{t-1}: \nexists j\in G_i \text{ such that }I^t_i\subseteq G^t_j\}$}.\]
\end{definition}
Otherwise speaking, an active Rebel in the $t$-block is a Rebel whose information about $\theta$, $I^t_i$, is not a subset of any other Rebel's same information. In the configuration in Figure~\ref{fig:circle_with_bridge_5} for Example~\ref{ex:leading}, the set of Rebels is $\{1,2,3,4,6\}$, the active Rebels in the $1$-block are Rebel 2 and 3, and no Rebel is active in the $t$-block when $t\geq 2$. %Take the configuration in Figure~\ref{fig:T-round-4} as another example, $R^0=\{1,2,3,4\}$, $R^1=\{2,3\}$, and $R^2=R^3=...=\emptyset$. 
Notice that the active Rebels have to be also the active ones in the previous block; they are fewer and fewer as $t$ goes by. Let us call a Rebel \textit{inactive Rebel} if he is not an active Rebel.

%\begin{figure}
%
%\begin{center}
%\begin{tikzpicture}[scale=0.7]
%    % Draw a 7,11 network
%    % First we draw the vertices
%    \foreach \pos/\name in {{(1,2)/R_1}, {(2.5,2)/R_2}, {(4,2)/R_3}, {(5.5,2)/R_4}}
%        \node[vertex] (\name) at \pos {$\name$};
%    
%%    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}}
%%    \node[unknown vertex] (\name) at \pos {$\name$};
%    
%%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%%    \node[selected vertex] (\name) at \pos {$\name$};
%    
%    % Connect vertices with edges 
%    \foreach \source/ \dest in {R_1/R_2, R_2/R_3, R_3/R_4}
%        \path[edge] (\source) -- (\dest) ;
%        
%\end{tikzpicture}
%\end{center}
%\caption{A configuration of the state and the network in which players 1, 2, 3, 4 are Rebels.}
%\label{fig:T-round-4}
%\end{figure}

It is sufficient to reveal the relevant information by letting only active Rebels share information about $\theta$, given that the network is acyclic and given that $\theta$ has strong connectedness. Theorem~\ref{lemma_empty} articulates this.
\begin{theorem}
\label{lemma_empty}
If the network is acyclic and if the $\theta$ has strong connectedness, then there is a strategy so that there exists an active Rebel in the $t$-block who can learn the relevant information at $t+1$-block.
\end{theorem}
Note that $I^t_i\in \mathbf{I}$. The following strategy is for Theorem~\ref{lemma_empty}.\footnote{Theorem~\ref{lemma_empty} is equivalent to the following statement: if the network is acyclic and if the $\theta$ has strong connectedness, then there exists $t\geq 0$ and $i\in R^t$ so that $I^{t+1}_i=[R](\theta)$.} 
\begin{center}
\textbf{Strategy for Theorem~\ref{lemma_empty}:}
\begin{enumerate}
\item In each reporting phase in the $t$-block, each active Rebel $i$ at $t$-block plays $\langle {I^t_i} \rangle\in W$.
\item At each period in each coordination phase in the $t$-block, all Rebels play \textbf{stay}.
\end{enumerate}
\end{center}
\begin{proof} 
Here, I show the proof that there is $t$ and there exists Rebel $i$ who can learn the relevant information in the $t+1$-block. The proof that $i$ has to be active in the $t$-block is in Appendix.  Following the above strategy, right after $t$-block, Rebel $i$ assigns probability one to the event 
\[\{\theta\in \Theta: \text{$\theta_j=R \Leftrightarrow j\in I^{t+1}_i$}\}\] 
by Bayesian rule. To conclude the proof, it is sufficient is to show there exists a $t$ so that $I^{t+1}_i=[R](\theta)$. By definition, $I^t_i$ is the set of Rebels who can be reached by at most $t$ consecutive edges from Rebel $i$, in each of which the endpoints are Rebels. Since $\theta$ has strong connectedness, there exists a $t_i$ so that $I^{t_i}_i=[R](\theta)$. What follows is $i$ learns $\theta$ at $t_i$.
\end{proof}
\begin{remark}
Theorem~\ref{lemma_empty} is not true if the network is cyclic. Take the configuration in Figure~\ref{fig:circle_with_bridge_5} for Example~\ref{ex:leading} as an example. By following Strategy for Theorem~\ref{lemma_empty}, in the $2$-block, there is no Rebel who learns the relevant information. Then, in the $3$-block, either Rebel 1 or 4 learns the relevant information, but both Rebel 1 and 4 are inactive at $2$-block. Theorem~\ref{lemma_empty} thus captures a free-rider problem in cyclic networks as the above mentioned: Rebel 1 and Rebel 4 are redundant mutually.
\end{remark}






%\begin{lemma}
%\label{lemma_inclusion}
%If the $\theta$ has strong connectedness, then 
%$R^t\subseteq R^{t-1}$ for all $t\geq 1$.
%\end{lemma}



Modifying Strategy for Theorem~\ref{lemma_empty}, Strategy for Proposition~\ref{lemma:small_apex} below is an APEX strategy.
\begin{proposition}
\label{lemma:small_apex}
If the network is acyclic and if the $\theta$ has strong connectedness, then there is an APEX strategy.
\end{proposition}
Consider the following strategy. 
\[\textbf{Strategy for Proposition~\ref{lemma:small_apex}:}\]
\begin{enumerate}
\item Suppose $T^{\theta}$ has arrived, Rebels play the ex-post efficient outcome. 
\item Suppose $T^{\theta}$ has not yet arrived. In the reporting phase, Rebels follow Strategy for Theorem~\ref{lemma_empty}. In the coordination phase, as below, the strategy is a contagion process. 
\begin{enumerate}
\item If a Rebel has been certain $\#[R](\theta)<k$, he plays sequence of actions $(\textbf{stay},\textbf{stay})$ continuously starting right after he was certain that. He plays \textbf{stay} forever after this phase; $T^{\theta}$ arrives right after this phase.
\item If a Rebel has learnt $\#[R](\theta)\geq k$, he plays sequence of actions $(\textbf{revolt},\textbf{revolt})$ continuously starting right after he learnt that. He plays \textbf{revolt} forever after this phase; $T^{\theta}$ arrives right after this phase.
\item If a Rebel has observed the sequence of actions $(\textbf{stay},\textbf{stay})$, he plays $(\textbf{stay},\textbf{stay})$ continuously starting right after he observed that and plays \textbf{stay} forever after this phase; $T^{\theta}$ arrives right after this phase.
\item If a Rebel has observed the sequence of actions $(\textbf{revolt},\textbf{revolt})$, he plays $(\textbf{revolt},\textbf{revolt})$ continuously starting right after he observed that and plays \textbf{revolt} forever after this phase; $T^{\theta}$ arrives right after this phase.
\item If a Rebel is uncertain $\#[R](\theta)\geq k$, he plays sequence of actions $(\textbf{revolt},\textbf{stay})$ continuously.
\end{enumerate} 
\end{enumerate}



This is a simple extension of Strategy for Example~\ref{ex:leading}. Rebels share information in reporting phase. If a Rebel has learnt the relevant information, he disseminates it to all Rebels contagiously in coordination phase; otherwise, he continues to the next phase---a reporting phase. 

If incentive compatibility is under consideration, this logic, however, brings another free-rider problem. Suppose that there are two Rebels who share information to each other in a reporting phase, and each of them is certain that he will learn the relevant information if the other one shares \textit{truthful} information to him. Because sharing information is done by alternating actions, which incurs expected payoff, they will not truthfully share their information. This is because each of them will choose his most profitable way of sharing information without impeding learning the relevant information provided that the other one share the truthful information. The free-rider problem turns out to be the main challenge in the construction of an APEX equilibrium. The proof solves it by arguing that if the network is acyclic, the free-rider problem only occurs between two Rebel neigbors who \textit{commonly know it}, while this argument does not hold for cyclic network.\footnote{Section~\ref{sec:cyclic} provides an example that the free-rider problem is not commonly known between the Rebels who involve.} 
With the help from this argument, the constructed equilibrium solves the free-rider problem by arbitrarily assigning one of them to be the {free rider}, who can choose his most profitable way in sharing information, while letting the other one share truthful information. 

To make the discussion of incentive compatibility more transparent, I introduce \textit{$T$-round writing game} as an auxiliary scenario. In the $T$-round writing game, $T$ is fixed, players are endowed a writing technology so that they can write to share information about $\theta$ for $T$ rounds. They then play a one-shot $k$-threshold game at round $T+1$. This game is a reduced form of the original game by fixing $T$, so that I can pay attention only to the incentive compatibility in information sharing process and ignore how players determine the terminal period. This writing technology represent a way of communication by \textit{writing sentences that are composed by letters according to a fixed grammar}. Though this auxiliary game will be soon introduced in the next section, I draw Table~\ref{table:analogue} to shed light on the parallel between it and the original game.

\begin{table}[!htbp]
\caption{The analogue between $T$-round writing game and the repeated $k$-threshold game}
\label{table:analogue}
\begin{center}
\begin{tabular}{cc }
$T$-round writing game & Repeated $k$-threshold game \\
\hline
\hline
A round & A range of periods\\
A sentence & A sequence of actions \\
The length of a sentence in a round & The length of a range of periods\\
A chosen letter in a sentence & A chosen action\\
The cost of writing a sentence & The expected payoff occurring in a sequence of actions\\
The fixed grammar & The equilibrium path\\
\hline
\end{tabular}
\end{center}

\end{table}


\subsection{$T$-round writing game}
\label{sec:writing}
The network, the set of states, and the set of players follow exactly the same definitions defined in Section~\ref{sec:model}. In the $T$-round writing game, each player endows a \textit{writing technology}. A writing technology for player $i$ is a pair of $(W,M_i)$, in which $W=\{{\textbf{r}},\textbf{s}\}^L$, $L\in \mathbb{N}$, and $M:= \bigtimes_{i\in N}\bigtimes^T_{t=1}M^{t}_i$ recursively defined by
\[M^1_i=\{f|f:\Theta_{G_i}\rightarrow W\}\cup \{\emptyset\}\]
\[ \text{ for } 2\leq t \leq T\text{ , }M^{t}_i=\{f|f:\bigtimes_{j\in G_i}M^{t-1}_j\rightarrow W\}\cup \{\emptyset\}. \]
$W$ is interpreted as the set of sentence composed by letters \textbf{r} or \textbf{s} with length $L$, while $M_i$ is understood as $i$'s grammar. $\emptyset$ represents remaining silent. The phrase of ``$i$ writes a sentence to all his neighbors at round $t$'' is equivalent to ``$i$ selects an $f\in M^t_i$ to get an element $w\in W$ according to $f$, which can be observed by all $i$'s neighbors''. 
%A sentence being a mixture of two sentences $w,w^{'}\in W$ is denoted as $w\oplus w^{'}$ with the property that $(w\oplus w^{'})_l=\textbf{r}$ if and only if $w_l=\textbf{r}$ or $w^{'}_l=\textbf{r}$ for all $l\in \{1,...,L\}$. 

The time line for the deterministic $T$-round writing game is as follows.
\begin{enumerate}
\item Nature chooses $\theta$ according to the prior $\pi$.
\item $\theta$ is then fixed throughout rounds.
\item At $t=1,...,T$ round, players write to their neighbors. 
\item At $T+1$ round, players play a one-shot $k$-Threshold game.
\item The game ends.
\end{enumerate}
%\begin{table}[!htbp]
%\begin{center}
%\begin{tabular}{ll }
%1.  & Nature chooses $\theta$ according to $\pi$. \\
%2. & $\theta$ is then fixed throughout rounds.\\
%3. & At $t=1,...,T$ round, players write to their neighbors. \\
%4. & At $T+1$ round, players play a one-shot $k$-Threshold game.\\
%5. & The game ends.
%\end{tabular}
%\end{center}
%\end{table}
A Rebel's payoff is the summation of his stage payoff across stages, while an Inert's payoff is set to be $1$. The equilibrium concept is weak sequential equilibrium in which the out-of-path belief is that a Rebel believes all players outside his neighborhood are Inerts. An APEX strategy is a strategy that induces the ex-post outcome in the one-shot $k$-threshold game at $T+1$ round. The definition of APEX equilibrium is adapted accordingly. In the examples below, let us focus on the configuration represented in Figure~\ref{fig:T-round-6} and Figure~\ref{fig:T-round-5} with $n=L=8$. I.e.~there are 8 players and the length of a sentence is also 8. Note that the differences between configurations in Figure~\ref{fig:T-round-6} and Figure~\ref{fig:T-round-5} are: (1) there are 6 Rebels in Figure~\ref{fig:T-round-6} but are 5 Rebels in Figure~\ref{fig:T-round-5}; (2) player 8 is a Rebel in Figure~\ref{fig:T-round-6} but he is an Inert in Figure~\ref{fig:T-round-6}.

\begin{figure}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(1,2)/R_1}, {(2,1)/R_2}, {(5,1)/R_6}, {(6,2)/R_8}, {(3,2)/R_4}, {(4,2)/R_5}}
        \node[vertex] (\name) at \pos {$\name$};
    
    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}}
    \node[unknown vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%    \node[selected vertex] (\name) at \pos {$\name$};
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_4, R_2/R_4,I_3/R_4,R_4/R_5, R_5/R_6, R_5/I_7, R_5/R_8}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which players 1, 2, 4, 5, 6, 8 are Rebels while players 3, 7 are Inerts.}
\label{fig:T-round-6}
\end{figure}

\begin{figure}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(1,2)/R_1}, {(2,1)/R_2}, {(5,1)/R_6}, {(3,2)/R_4}, {(4,2)/R_5}}
        \node[vertex] (\name) at \pos {$\name$};
    
    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}, {(6,2)/I_8}}
    \node[unknown vertex] (\name) at \pos {$\name$};
    
%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%    \node[selected vertex] (\name) at \pos {$\name$};
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_4, R_2/R_4,I_3/R_4,R_4/R_5, R_5/R_6, R_5/I_7, R_5/I_8}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which players 1, 2, 4, 5, 6 are Rebels while players 3, 7, 8 are Inerts.}
\label{fig:T-round-5}
\end{figure}

%
%\subsubsection{$T$-round writing game with cheap talk}
%\label{sec:cheap_talk}
%\begin{example}[Deterministic $T$-round writing without cost---cheap talk]
%\label{ex:cheap_talk}
%Let $k=6$, $T=2$, and suppose writing is costless. This is, there are two rounds for writing, while writing is the same as cheap talk.
%%Let the grammar for Rebel 4 (or 5) be: \textbf{r} is written in the $i$-th component if player $i$ is a Rebel; \textbf{s} is written in the $j$-th component if $j$'s type is an Inert or unknown to Rebel 4 (or 5).
%
%Consider the following strategy $\phi$. 
%
%At $t=1$, the peripheral Rebels remain silent. Rebel 4 (or 5)'s grammar for writing a sentence is: if player $i$ is a Rebel and known to him, he writes \textbf{r} in the $i$-th component in the sentence; otherwise, he writes \textbf{s} in that component. According to this grammar, the central player Rebel 4 writes the sentence $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{s},\textbf{s},\textbf{s})$ on both configurations in Figure~\ref{fig:T-round-6} and in Figure~\ref{fig:T-round-5}. The central player Rebel 5 writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{r})$ in the configuration in Figure~\ref{fig:T-round-6} and writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$ in the configuration in Figure~\ref{fig:T-round-5}.\footnote{The notion of ``peripheral'' and ``central'' will be formalized in Section~\ref{sec:info}} Rebels 4 and 5's sentences thus reveals who are Rebels and who are not. Notice that the common knowledge of the network contributes to the ability of revealing players' types. 
%
%At $t=2$, the peripheral Rebels still remain silent. Rebel 4 writes $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{r})$ in the configuration in Figure~\ref{fig:T-round-6} and writes $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$ in the configuration in Figure~\ref{fig:T-round-5}. Rebel 5 writes exactly the same sentence as Rebel 4. This is to say Rebel 4 and 5 share information at $t=1$ and then coordinate to announce a mixture sentence at $t=T$. 
%
%At $T+1$, by counting \textbf{r} in Rebel 4 or 5's mixture sentence, all Rebels know whether the number of Rebels outnumbers $k$. This leads all Rebels to play the ex-post efficient outcome in the one-shot $k$-threshold game. 
%
%%Suppose the configuration is that in Figure~\ref{fig:T-round-5}. At $t=1$, similarly, $\phi$ specifies that Rebels 1,2,6,8 keep silent; Rebel 4 writes $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{s},\textbf{s},\textbf{s})$; Rebel 5 writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$. At $t=2$, however, $\phi$ specifies that the peripheral Rebels 1,2,6,8 keep silent; Rebel 4 keeps silent; Rebel 5 keeps silent as well. On the path, keeping silent by Rebel 4 (or 5) reflects that Rebel 4 (or 5) knows that the total number of Rebels is less than $k=6$. At $t=3$, all Rebels know this relevant information and play the ex-post efficient outcome \textbf{stay}.
%
%
%\end{example}
%
%\begin{remark}
%An assessment $(\phi,\omicron)$, where $\omicron$ is a belief system, is made to be an APEX equilibrium for Example~\ref{ex:cheap_talk} above as follows. The out-of-path strategy of $\phi$ specifies that if there is a detectable deviation, then the Rebels who detect this deviation will remain silent until $t=T$ and then play \textbf{stay} at $T+1$.\footnote{A deviation could be, for instance, a wrong sentence that is not grammatical, is deviating from the in-path $\phi$,..., etc} 
%The out-of-path belief of $\omicron$ is to believe that all players who are not neighbors are Inerts. The in-path belief of $\omicron$ is the belief induced by $\phi$. Since any deviation by Rebel 4 or 5 would strictly decrease the possibility of achieving an ex-post efficient outcome, and writing is costless, the assessment $(\phi,\omicron)$ constitutes an APEX equilibrium.
%\end{remark}
%\begin{example}[Deterministic $T$-round writing with a fixed cost]
%\label{ex:fixed_cost_talk}
%Let $k=6$ and $T=2$. Suppose that writing incurs a fixed cost $\epsilon>0$ while keeping silent does not. Let us consider the assessment $(\phi,\omicron)$ in the above example. Since any deviation by Rebel 4 or 5 would strictly decrease the possibility to achieve the ex-post efficient outcome while the ex-post efficient outcome will give the maximum expected payoff for every Rebel at $t=3=T+1$ if the relevant information can be revealed then, if $\epsilon$ is sufficiently small, $(\phi,\omicron)$ also constitutes an APEX equilibrium.
%
%\end{example}

%If writing is costly instead, the next example shows that $(\phi,\omicron)$ is not an APEX equilibrium. This is due to the free-rider problem.

\begin{example}[$T$-round writing and the free-rider problem]
\label{ex:cost_function_talk_fr}
Let $k=6$ and $T=2$. Suppose that remaining silent incurs no cost, but writing incurs an extremely small cost $\epsilon>0$ so that $\epsilon$ is strictly decreasing with the number of \textbf{r} in a sentence. This is to say writing $(\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{r})$ incurs the least cost while writing $(\textbf{s},\textbf{s},\textbf{s},\textbf{s},\textbf{s},\textbf{s},\textbf{s},\textbf{s})$ incurs the largest.\footnote{This cost is not directly linking to the expected payoff in the original game, but is meant to be a metaphor to emphasize that alternating actions might incurs negative expected payoff in the original game and to illustrate the potential free-rider problem.} 

Let us consider the following strategy $\phi$. 

At $t=1$, the peripheral Rebels remain silent. Rebel 4 (or 5)'s grammar for writing a sentence is that if player $i$ is a Rebel and known to him, he writes \textbf{r} in the $i$-th component in the sentence; otherwise, he writes \textbf{s} in that component. According to this grammar, the central player Rebel 4 writes the sentence $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{s},\textbf{s},\textbf{s})$ on both configurations in Figure~\ref{fig:T-round-6} and in Figure~\ref{fig:T-round-5}. The central player Rebel 5 writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{r})$ in the configuration in Figure~\ref{fig:T-round-6} and writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$ in the configuration in Figure~\ref{fig:T-round-5}. Rebels 4 and 5's sentences thus reveals who are Rebels and who are not. Notice that the common knowledge of the network contributes to the ability of revealing players' types. 

At $t=2$, the peripheral Rebels still remain silent. Rebel 4 writes $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{r})$ in the configuration in Figure~\ref{fig:T-round-6} and writes $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$ in the configuration in Figure~\ref{fig:T-round-5}. Rebel 5 writes exactly the same sentence as Rebel 4. This is to say Rebel 4 and 5 share information at $t=1$ and then coordinate to announce a mixture sentence at $t=T$. 

At $T+1$, by counting \textbf{r} in Rebel 4 or 5's mixture sentence, all Rebels know whether the number of Rebels outnumbers $k$. This leads all Rebels to play the ex-post efficient outcome in the one-shot $k$-threshold game. 

The above $\phi$ is not an APEX equilibrium. According to the above-mentioned, Rebel 4 will know the relevant information at $t=2$ even if he deviates to writing that all his neighbors are Rebels, which incurs less cost than his truthful writing.\footnote{This sentence is $(\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s},\textbf{s})$, which incurs less cost than the truthfully reporting sentence $(\textbf{r},\textbf{r},\textbf{s},\textbf{r},\textbf{r},\textbf{s},\textbf{s},\textbf{s})$.}\footnote{If he remains silent, then this behavior will be considered as a deviation, and therefore he will never get the maximum payoff of $1$. Hence, he will avoid doing so.}
Rebel 5 is in the same situation as Rebel 4 and therefore also writes the sentence that indicates that all his neighbors are Rebels. However, these sentences are uninformative. It turns out that both of them will deviate, and neither of them can know the relevant information at $t=2$.
%\footnote{Actually, one can claim there is no APEX equilibrium in this case by contraposition. If there is one, say $(\phi^{'},\omicron^{'})$, then on the path of $\phi^{'}$, some Rebels should write and play differently in different configurations. Suppose Rebel 5 will play differently to truthfully reveal his information at $t=1$. Rebel 4 has two kinds of strategies then. He might write the least-cost sentence or he could write something depending on his information. For the former case, since Rebel 4 will be the only one who know the relevant information, he should write to Rebel 5 to let Rebel 5 pass this information to the remaining Rebels. However, it requires two rounds to pass it, but there is only one round left. For the latter case, if Rebel 4 truthfully reveal his information, the Rebel 5 will deviate to writing least-cost sentence; if Rebel 4's information is not truthfully reveal, there is a prior so that the ex-post outcome will not be played.}


\end{example}

Fortunately, the following example shows that the free-rider problem can be solved. 

\begin{example}[$T$-round writing and solving the free-rider problem]
\label{ex:cost_function_talk_solve_fr}
The solution to solve the free-rider problem in the previous example is to extend $T$. It would open the possibility of the existence of a free rider at some round, while letting this free rider reveals relevant information at the next round. To this end, let $k=6$ and $T=3$. Consider the following strategy $\rho$ and focus on the interaction between Rebels 4 and 5.
%First note that if the previous strategy $\phi$ is extended to the current case so that Rebel 4 and 5 still simultaneously truthfully write their information at either $t=1$ or $t=2$, $\phi$ is not an equilibrium path by the same free-rider argument as the above. 

At $t=1$, the lowest-index Rebel between Rebels 4 and 5 is the free rider, while the other one truthfully writes down his information. This is to say Rebel 4 will be the free rider and he writes the least-cost sentence. Rebel 5 writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{r})$ in the configuration of Figure~\ref{fig:T-round-6}and writes $(\textbf{s},\textbf{s},\textbf{s},\textbf{r},\textbf{r},\textbf{r},\textbf{s},\textbf{s})$ in the configuration of Figure~\ref{fig:T-round-5}. 

At $t=2$, Rebel 4 has known the relevant information. Rebel 4 writes the least-cost sentence if $\#[R](\theta)\geq k$ but remains silent otherwise. The consequence is Rebel 4's behavior reveals the relevant information to his neighbors at this round. Rebel 5 remains silent instead. 

At $t=T$, Rebel 5 has known the relevant information since he is Rebele 4's neighbor. He writes the least-cost sentence if $\#[R](\theta)\geq k$ but remains silent otherwise. Therefore Rebel 5's behavior at this round reveals the relevant information to his neighbors. Rebel 4 remains silent instead.

At $T+1$, all Rebels know the relevant information by observing Rebels 4 and 5's behavior. They play the ex-post efficient outcome accordingly. 
%To construct an APEX equilibrium from $\rho$, recall $(\phi,\omicron)$ and let the in-path belief of $\dot{\omicron}$ be induced by $\rho$. The out-of-path strategy follows that in $\phi$, and the out-of-path belief follows that of $\omicron$.   



\end{example}

\begin{remark}
Why does Rebel 5 \textit{know} that he is not a free rider and therefore behaves not like a free rider? The following is the reason. He \textit{knows} that, by common knowledge of the network, he and Rebel 4 are in a free-rider problem. Moreover, by common knowledge of the network, he knows that Rebel 4 knows that he and Rebel 4 are in a free-rider problem, he knows that Rebels 4 knows that he knows that,...,and so forth. Consequently, Rebel 5 and 4 commonly know that they are engaged in a free-rider problem. Lemma~\ref{lemman_pivotals_CK} articulate a general result: engaging in a free-rider problem is a common knowledge between Rebels who involve for any acyclic network in the constructed APEX equilibrium. Lemma~\ref{lemman_pivotals_CK} is, however, false in cyclic networks. Section~\ref{sec:cyclic} discusses this issue.
\end{remark}

\subsection{Equilibrium construction}
\label{sec:dis_writing}
I then delver into substantial ideas in constructing an APEX equilibrium in the following sections, while leaving actual details in Appendix. The framework is adapted from Proposition~\ref{lemma:small_apex}, and the time is partitioned by two consecutively alternating phases as shown in Figure~\ref{fig:ordered_original_game_intro}. Recall that the time is partitioned into
\[0<\underbrace{(\text{coordination phase})<(\text{reporting phase})}_{\text{$1$-block}}<\underbrace{(\text{coordination phase})<(\text{reporting phase})}_{\text{$2$-block}}<...\]

The out-of-path belief is simple and serves as a grim trigger. Whenever Rebel $i$ detects a deviation at period $\varsigma$, he forms the following belief: 
\begin{equation}
\label{eq_grim_trigger}
\sum_{\theta \in \{\theta:\theta_j=I,j\notin G_i\}}\beta^{\pi,\tau}_{G_i}({\theta}|h^{s}_{G_i})=1 \text{, for all $s\geq \varsigma$}.
\end{equation}
This is to say $i$ believe all players outside his neighborhood are Inerts. Thus, if $\# I^{\varsigma}_i<k$, he will play \textbf{stay} forever after he detects a deviation. 

\subsubsection{The equilibrium path in the reporting phase}
\label{sec:eq_rp}
The description in this section is for the APEX equilibrium path {before} $T^{\theta}$. Let us shorten ``reporting phase in $t$-block'' by $\Omicron^{t}$, denote $|\Omicron^t|$ as the length of $\Omicron^{t}$, and shorten \textbf{revolt} and \textbf{stay} to \textbf{r} and \textbf{s} receptively. For simplicity, the term ``reporting phase'' refers to ``reporting phase in $t$-block'' in this section.

$|\Omicron^{t}|$ is independent from $t$ and determined only by the set of players. Firstly, assign each player $i$ a distinct prime number $x_i$ starting from $3$. Then let $|\Omicron^{t}|=\bigotimes_{i\in N} x_i=x_1\otimes x_2\otimes...\otimes x_n$ , where $\otimes$ is the usual multiplication operator. The sequence of actions in $\Omicron^{t}$ is with length $|\Omicron^t|$ and would take one of the forms specified in the right column in Table~\ref{Table_msg_form}. The abbreviations of these sequences are listed in the left column. Since these sequences in the reporting phase are meant to share information about $\theta$, the terms ``playing the sequence'' and ``reporting the information'' are interchangeable and will be alternatively used.


\begin{table}[!htbp]
\caption{The notations for the sequences of actions in $\Omicron^t$ on the path}
\label{Table_msg_form}
\begin{center}
\begin{tabular}{l c c}
Notations && The sequences of actions\\
\hline
\hline
$\langle  I \rangle$ 				& $:=$ 			& $(\overbrace{\textbf{s},...,\textbf{s},\underbrace{\textbf{r},\textbf{s},...,\textbf{s}}_{\bigotimes_{j\in I}x_j} }^{\bigotimes_{i\in N} x_i})$  \\
$\langle 1 \rangle$	 					& $:=$ 			& $( \overbrace{\textbf{s},...,\textbf{s},{\textbf{r}} }^{\bigotimes_{i\in N} x_i} )$  \\
$\langle \textbf{all stay} \rangle$	 					& $:=$ 			& $(\overbrace{ \textbf{s},...,\textbf{s},{\textbf{s}} }^{\bigotimes_{i\in N} x_i})$  \\
\hline
\end{tabular}
\end{center}
\end{table}


It is worth noting that the sequence constructed by prime numbers brings two benefits. Firstly, since the multiplication of distinguishing prime numbers can be uniquely factorized, the Rebels can use such sequence to precisely report players' identities. Secondly, the un-discounted expected payoff of playing $\langle I^t_i \rangle$ for some $I^t_i$ for an active Rebel $i$ is always equal to $-1$, and therefore it is relatively easy to calculate. This is because only active Rebels will report $\langle I \rangle$ for some $I$. Since this $I$ is not reported by any other Rebel, at most one Rebel would play \textbf{r} at any period in the reporting phase by the property of prime number multiplication.\footnote{This statement holds if there is no Rebel who plays $\langle 1 \rangle$.} 

I list the sequences played in the reporting phase on the path in Table~\ref{Table_msg_RP_path}, in which the terms of $\theta$-pivotal, $k-1$-pivotal, and free-rider problem will be formally defined one-paragraph later.

\begin{table}[!htbp]
\caption{The sequences of actions played in $\Omicron^t$ on the path}
\label{Table_msg_RP_path}
\begin{center}
\begin{tabular}{l c}
Rebel $i$ & $i$ plays\\
\hline
\hline
$i$ is inactive				& $\langle \textbf{all stay} \rangle$  \\
$i$ is active but $i$ is not pivotal	 					 			& $\langle I^t_i \rangle$  \\
$i$ is $k-1$-pivotal	 					 			& $\langle 1 \rangle$  \\
$i$ is $\theta$-pivotal but not in the free-rider problem	 					 			& $\langle 1 \rangle$  \\
$i$ is in the free-rider problem with the lowest index	 					 			& $\langle 1 \rangle$  \\
$i$ is in the free-rider problem without the lowest index	 					 			& $\langle I^t_i \rangle$  \\
\hline
\end{tabular}
\end{center}
\end{table}

On the path, the sequences $\langle I \rangle$ or $\langle 1 \rangle$ are meant to differentiate themselves from $\langle \textbf{all stay} \rangle$. The sequence $\langle \textbf{all stay} \rangle$ is for the inactive Rebels at $t$-block to report nothing. The sequence $\langle I \rangle$ is for active Rebels at $t$-block to report $I$ if $I$ is a set of Rebels. Although the definitions of \textit{pivotal Rebel} and \textit{free-rider problem} has not yet formally defined at this present, the sequence $\langle 1 \rangle$ is intentionally crafted to tackle the free-rider problem. To see how $\langle 1 \rangle$ works, I turn to formally defining the {pivotal Rebel} and the {free-rider problem}. 

\begin{definition}[Pivotal Rebels in $\Omicron^t$]
A Rebel $p$ is pivotal in $\Omicron^t$ if $p$ is active at $t$-block, $p$ is uncertain the relevant information, and $p$ is certain that he will learn the relevant information right after $\Omicron^t$, given that each $i\in R^t$ reports $\langle I^t_i \rangle$.
\end{definition}

By the definition, a pivotal Rebel in the reporting phase is one who can learn the relevant information if all of his active Rebel neighbors truthfully report their information about $\theta$ to him. The pivotal Rebels can be further classified into two kinds: ones who can learn $\theta$, and ones who learn only the relevant information. When $k=6$, in the configuration in Figure~\ref{fig:T-round-6}, only Rebels 4 and 5 are pivotal; they are of the first kind. In the configuration in Figure~\ref{fig:central_pivotal}, only Rebel 5 is pivotal; he is of the first kind. In the configuration in Figure~\ref{fig:k-1_pivotal}, only Rebel 4 pivotal; he is of the second kind.

%\begin{figure}
%
%\begin{center}
%\begin{tikzpicture}[scale=0.7]
%    % Draw a 7,11 network
%    % First we draw the vertices
%    \foreach \pos/\name in {{(1,2)/R_1}, {(2,1)/R_2}, {(5,1)/R_6}, {(6,2)/R_8}, {(3,2)/R_4}, {(4,2)/R_5}}
%        \node[vertex] (\name) at \pos {$\name$};
%    
%    \foreach \pos/\name in {{(2,3)/I_3}, {(5,3)/I_7}}
%    \node[unknown vertex] (\name) at \pos {$\name$};
%    
%%    \foreach \pos/\name in {{(3,2)/S_4}, {(4,2)/S_5}}
%%    \node[selected vertex] (\name) at \pos {$\name$};
%    
%    % Connect vertices with edges 
%    \foreach \source/ \dest in {R_1/R_4, R_2/R_4,I_3/R_4,R_4/R_5, R_5/R_6, R_5/I_7, R_5/R_8}
%        \path[edge] (\source) -- (\dest) ;
%        
%\end{tikzpicture}
%\end{center}
%\caption{A configuration of the state and the network in which players 1, 2, 4, 5, 6, 8 are Rebels while players 3, 7 are Inerts.}
%\label{fig:T-round-6}
%\end{figure}


\begin{figure}

\begin{center}
\begin{tikzpicture}[scale=1]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(1,1)/R_1}, {(1,3)/I_2}, {(2,2)/R_3}, {(3,1)/R_4}, {(3,2)/R_5}, {(3,3)/I_6}, {(4,2)/R_7}, {(5,1)/R_8}, {(5,3)/I_9}}
        \node[vertex] (\name) at \pos {$\name$};
    
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_3, I_2/R_3, R_3/R_5, R_4/R_5, I_6/R_5, R_5/R_7, R_7/R_8, R_7/I_9}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which player 1, 3, 4, 5, 7, 8 are Rebels while players 2, 4, 9 are Inerts.}
\label{fig:central_pivotal}
\end{figure}

\begin{figure}


\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(2,2)/R_1}, {(2,1)/R_2}, {(2,3)/R_3}, {(3,2)/R_4}, {(4,2)/R_5}, {(5,2)/R_6}, {(6,2)/R_7}, {(7,2)/I_8}}
        \node[vertex] (\name) at \pos {$\name$};
    
    
    % Connect vertices with edges 
    \foreach \source/ \dest in {R_1/R_4, R_2/R_4,R_3/R_4,R_4/R_5, R_5/R_6, R_6/R_7, R_7/I_8}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which player 1, 2, 3, 4, 5, 6, 7 are Rebels while player 8 is an Inert.}
\label{fig:k-1_pivotal}
\end{figure}

Call $p$ of the first kind the \textit{$\theta$-pivotal} Rebel. For the second kind, if the network is acyclic and if the prior has full support on strong connectedness, $p$ is the second kind only if $I^{t}_p=k-1$. Call this second kind \textit{$k-1$-pivotal} Rebel.\footnote{To show that a pivotal Rebel is the second kind in $\Omicron^{t}$ only if $I^{t}_p=k-1$, one can follow the same argument in Theorem~\ref{lemma_empty}.} Below is the defined free-rider problem in the reporting phase.

\begin{definition}
A free-rider problem exists in $\Omicron^t$ if there are multiple $\theta$-pivotal Rebels in $\Omicron^t$.
\end{definition}



The following lemma is crucial. 
%it says that there will be at most two $\theta$-pivotal Rebels in $R^{t}$. Therefore, if there is a free-rider problem, it occurs between two $\theta$-pivotal Rebels. Moreover, if there are two of them, they are neighbors. (... it occurs between two Rebels who are neighbors.)
\begin{lemma}
\label{lemma_at_most_two_nodes}
If the network is acyclic and if $\pi$ has full support on strong connectedness, there are at most two $\theta$-pivotal Rebels in the $t$-block. Moreover, they are neighbors.\footnote{As a remark, Lemma~\ref{lemma_at_most_two_nodes} is not true when the network is cyclic. To see this, consider a 4-player circle given that $\theta=(R,R,R,R)$.}
\end{lemma}

And notably,

\begin{lemma}
\label{lemman_pivotals_CK}
If the network is acyclic and if $\pi$ has full support on strong connectedness, when there are two $\theta$-pivotal Rebels $p,p^{'}$ in the $t$-block, then they commonly know that they are $\theta$-pivotal Rebels at the beginning of $t$-block.
\end{lemma}

By Lemma~\ref{lemman_pivotals_CK}, $\theta$-pivotal Rebels in the reporting phase can identify themselves at the beginning of this phase. This importance cannot be further emphasized. If the free-rider problem will occurs in the reporting phase, the strategy can, before the reporting phase, specify that the lowest indexed $\theta$-pivotal Rebel $p$ involving in the free-rider problem will play $\langle 1 \rangle$, while the other one will play $\langle I^t_{p^{'}} \rangle$. Otherwise speaking, this knowledge is encoded in the belief system of an APEX equilibrium. 

\begin{remark}
It is worth noting that the assumption of acyclic network in Lemma~\ref{lemman_pivotals_CK} is indispensable. Lemma~\ref{lemman_pivotals_CK} does not hold if the network is cyclic. Section~\ref{sec:cyclic} demonstrates it.
\end{remark}






\subsubsection{The equilibrium path in the coordination phase}
\label{sec:eq_cd}
The descriptions in this section is for the APEX equilibrium path {before} $T^{\theta}$. The term ``coordination phase in $t$-block'' is shorten by $\Kappa^{t}$.  

It is elaborate to spell out the coordination phase structure, but this phase is actually a simple contagion scenario: Rebels jointly decide to terminate or continue their information sharing during this phase. For that, a coordination phase is partitioned into three \textit{divisions}. In the first division, if there is a Rebel has learnt that $\#[R](\theta)<k$, all Rebels will play \textbf{stay} forever right after this division, and $T^{\theta}$ arrives; otherwise, they continue to the next one. In the second division, if there is a Rebel has learnt that $\#[R](\theta)\geq k$, a portion of Rebels, at least $k$ Rebels, will play \textbf{revolt} forever right after this division; otherwise, they continue to the next one. In the third division, if there is a Rebel has learnt that $\#[R](\theta)\geq k$ in the previous divisions, all Rebel will play \textbf{revolt} forever right after this division, and $T^{\theta}$ arrives; otherwise, they continue to the next phase---a reporting phase.

To fulfil the above contagion argument, a set of sequences of actions played on the path is specified so that Rebels update their belief according to it. For this task, further partition a division into \textit{sub-blocks}. I depict the whole partition in the coordination phase below, where $\Box$ represents a sub-block in a coordination phase. 

In $\Kappa^1$, 
\[\overbrace{\underbrace{ \Box }_{\text{one sub-block}}}^{\text{$1$-division}} \overbrace{\underbrace{\Box }_{\text{one sub-block}}}^{\text{$2$-division}} \overbrace{\underbrace{\Box \cdot \cdot \cdot \Box}_{\text{$n$ sub-blocks}}}^{\text{$3$-division}}\] 

In $\Kappa^t$, $t\geq 2$,
\[\overbrace{\underbrace{\Box \cdot \cdot \cdot \Box}_{\text{$n$ sub-blocks}}}^{\text{$1$-division}} \overbrace{\underbrace{\Box \cdot \cdot \cdot \Box}_{\text{$t+1$ sub-blocks}} }^{\text{$2$-division}} \overbrace{\underbrace{\Box \cdot \cdot \cdot \Box}_{\text{$n$ sub-blocks}}}^{\text{$3$-division}}\] 




For each $t$, denote $\Kappa^t(u,\cdot)$ as the $u$-division and $|\Kappa^t(u,\cdot) |$ as the length of $\Kappa^t(u,\cdot)$. Likewise, denote $\Kappa^t(u,v)$ as the $v$-th sub-block in $u$-division and $|\Kappa^t(u,v) |$ as the length of $\Kappa^t(u,v)$. Let us shorten \textbf{revolt} and \textbf{stay} to \textbf{r} and \textbf{s} receptively. Let $|\Kappa^t(u,v)|=n$ if $u=1,2$, $v=1,...,n$. Let $|\Kappa^t(u,v)|=1$ if $u=3$, $v=1,...,n$. I list the the sequences of actions on the path and their notations in Table~\ref{Table_msg_coordination}, where $i$ is the original index of player $i$.\footnote{In the $3$-division, since the sequence of actions which length is 1 is equivalent to playing a single action, I do not provide additional notations for conciseness.}



\begin{table}[!htbp]
\caption{The notations for the sequences of actions in $\Kappa^t(u,v)$ for $u=1,2$, $v=1,...,n$, on the path}
\label{Table_msg_coordination}
\begin{center}
\begin{tabular}{l c c}
Notations & &The sequences of actions \\
\hline
\hline
$\langle i \rangle$ 				& $:=$ 			& $(\overbrace{ \textbf{s},...,\textbf{s},\underbrace{\textbf{r},\textbf{s},...,\textbf{s}}_{i}}^{n} )$  \\
$\langle \textbf{all stay} \rangle$	 					& $:=$ 			& $( \overbrace{\textbf{s},...,\textbf{s},{\textbf{s}}}^{n} )$  \\
\hline
\end{tabular}
\end{center}
\end{table}


\subsubsection{The equilibrium behavior on the path in $\Kappa^1$}
\label{sec:cd0}
I begin with depicting the equilibrium path in $\Kappa^1$, which is shown in Table~\ref{Table_cd0}. The belief updating after $\Kappa^1(1,\cdot)$ and $\Kappa^1(2,\cdot)$ on the path is listed in Table~\ref{Table_blf_cd0}. The evolution of players' information filtrations can be tracked throughout in this table. Since there is only one sub-block in $\Kappa^1(1,\cdot)$ or $\Kappa^1(2,\cdot)$, $\Kappa^1(1,\cdot)$ or $\Kappa^1(2,\cdot)$ is interchangeable with $\Kappa^1(1,1)$ or $\Kappa^1(2,1)$ respectively. If there is no confusing, the term ``coordination phase'' refers to ``coordination phase in the $1$-block'' in this section.

\begin{table}[!htbp]
\caption{The sequences of actions played in $\Kappa^1$ on the path}
\label{Table_cd0}
\begin{center}
\begin{tabular}{l c}
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^1(1,\cdot)$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
\hline
\\
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^1(2,\cdot)$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $\langle \textbf{all stay} \rangle$  \\
\hline
\\
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^1(3_v)$, where $v=1,...,n$, on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$ \textbf{s} $	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$ \textbf{s} $	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $ \textbf{s} $  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $ \textbf{r} $  \\
\hline
\end{tabular}
\end{center}
\end{table}

In the first division in the coordination phase, if Rebel $i$ is certain $\#[R](\theta)<k$, $i$ play $\langle \textbf{all stay} \rangle$. It implies that $i$ is certain that there is no Rebels outside $G_i$ and therefore learns $\theta$ by strong connectedness. This is to say all Rebels are $i$'s neighbors and thus all $i$'s Rebel neighbors are inactive. Since $\langle \textbf{all stay} \rangle$ is played only by an inactive Rebel or a Rebel who is certain $\#[R](\theta)<k$, all $i$'s Rebel neighbors learn $\#[R](\theta)<k$ right after the first division in the coordination phase. Since all Rebels are $i$'s neighbors, all Rebels learn $\#[R](\theta)<k$ right after the first division in the coordination phase. $T^{\theta}$ arrives then. Likewise, if Rebel $i$ is inactive and all $i$'s neighbors play $\langle \textbf{all stay} \rangle$, all Rebels learn $\#[R](\theta)<k$ right after the first division in the coordination phase, and $T^{\theta}$ arrives.    

In the second division in the coordination phase, there is a non-trivial construction: how Rebel $i$ disseminates the knowledge about $\#[R](\theta)\geq k$ if $i$ has learnt that. Rebel $i$ does so by playing $\langle i \rangle$ in the first division in the coordination phase and then play $\langle \textbf{all stay} \rangle$ in the second division in the coordination phase. His behavior is thus different from other kinds of Rebels. His neighbors will know $\#[R](\theta)\geq k$ right after the second division in the coordination phase and play \textbf{r} forever afterwards. Other Rebels will observe \textbf{r} being played in the third division in the coordination phase and thus know $\#[R](\theta)\geq k$ as well. All Rebels learn $\#[R](\theta)\geq k$ right after the third division in the coordination phase, and $T^{\theta}$ arrives.

Note that Rebel $i$ who has learnt $\#[R](\theta)\geq k$ will not deviate to play $\langle \textbf{all stay} \rangle$ in the first division in the coordination phase even though it might be undetectable. This is because the network is acyclic. If $i$ does so, $i$ will be considered as an inactive Rebel by all his neighbors from the point onwards right after the first division in the coordination phase. Two consequences follow. The first is that each of $i$'s neighbor is certain there is no more Rebel ``behind'' $i$.\footnote{To be more precise, this is to say there is no more Rebel in a sub-tree which excludes $j$ and roots at $i$.} 
The second is that $i$ keeps reporting nothing in the forthcoming reporting phases. Thus, all $i$'s neighbors report strictly less Rebels than they are supposed to do if $i$ follows the equilibrium path. $i$ then faces the possibility that no Rebel can know $\#[R](\theta)\geq k$ even if the total number of Rebels indeed exceeds $k$. If this event happens, $i$ will only get zero payoff. However, $i$ can surely get stage-game payoff as $1$ afterwards right after the second division in the coordination phase. Sufficiently high $\delta\in(0,1)$ will deter this deviation. 




\begin{table}[!htbp]
\caption{In $\Kappa^1$, on the path, the belief of $i$'s neighbor $j$ after observing $i$'s previous actions.}
\label{Table_blf_cd0}
\begin{center}
\begin{tabular}{l  c | c}
 	$i$ plays	  			&	  &  The event to which $j$ assigns probability one  right after $\Kappa^1(1,\cdot)$\\
\hline
\hline
In $\Kappa^1(1,\cdot)$	&		&		  \\
\hline
  $\langle \textbf{all stay} \rangle$	& &   $i$ is inactive if $j$ is inactive \\
  $\langle \textbf{all stay} \rangle$	&  &  $\#[R](\theta)< k$ if $j$ is inactive\\
  $\langle i \rangle$	&	&  $i$ is active or $\#[R](\theta)\geq k$    \\
  \hline
  \\
 	$i$ plays	  	&  	  &The event to which $j$ assigns probability one  right after $\Kappa^1(2,\cdot)$\\
\hline
\hline
	In $\Kappa^1(1,\cdot)$		&			In $\Kappa^1(2,\cdot)$	&  \\
\hline
  $\langle \textbf{all stay} \rangle$	&  $\langle \textbf{all stay} \rangle$ &  $i$ is inactive if $j$ is inactive \\
  $\langle \textbf{all stay} \rangle$	&  $\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)< k$ if $j$ is inactive\\
  $\langle i \rangle$	&	$\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)\geq k$    \\
  $\langle i \rangle$	&	$\langle i \rangle$ &  $i$ is active  \\
  \hline
\end{tabular}
\end{center}
\end{table}




\subsubsection{The equilibrium behavior on the path in $\Kappa^t$ for $t\geq 2$}
\label{sec:cdt}
The in-path strategy contingent on players' belief is introduced in Table~\ref{Table_cdt}. The evolution of information filtrations can be tracked throughout in Table~\ref{Table_blf_cdt}. The delicate part in $\Kappa^t$ is how a pivotal Rebel $p$ in $\Omicron^{t-1}$ disseminates the relevant information. For convenience, in this section, let $I^{t+1}_{ij}=I^t_i\cap I^t_j$ and the term ``coordination phase'' refer to ``coordination phase in the $t$-block''. I begin with the case when $p$ is certain $\#[R](\theta)< k$.

\begin{table}[!htbp]
\caption{The sequences of actions played in $\Kappa^t$, $t\geq 2$ on the path}
\label{Table_cdt}
\begin{center}
\begin{tabular}{l c}
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^t(1,v)$ for $t\geq 2$ and for $v=1,2,...,n$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$\langle i \rangle$	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
\hline
\\
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^t(2,v)$ for $t\geq 2$ for $v=1$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $\langle \textbf{all stay} \rangle$  \\
\hline
\\
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^t(2,v)$ for $t\geq 2$ for $v=2,...,t+1$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$\langle \textbf{all stay} \rangle$	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $\langle \textbf{all stay} \rangle$  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $\langle i \rangle$  \\
\hline
\\
\multicolumn{2}{c}{The sequences of actions played in $\Kappa^t(3,\cdot)$ for $t\geq 2$ on the path}\\
Rebel $i$ 	 	&  	$i$ plays		 \\
\hline
\hline
$i$ is certain $\#[R](\theta)<k$ 	& 	$\textbf{s}$	\\
$i$ is inactive and is uncertain $\#[R](\theta)\geq k$	& 	$ \textbf{s} $	\\
$i$ is active and is uncertain $\#[R](\theta)\geq k$ &  $ \textbf{s} $  \\
$i$ is certain $\#[R](\theta)\geq k$ &  $ \textbf{r} $  \\
\hline
\end{tabular}
\end{center}
\end{table}


%\clearpage
%\begin{landscape}
\begin{table}[!htbp]
\caption{In $\Kappa^t$, on the path, the belief of $i$'s neighbor $j$ after observing $i$'s previous actions.}
\label{Table_blf_cdt}
\begin{center}
\begin{tabular}{l l l | c}
%\multicolumn{4}{c}{The belief of $j\in G_i$ after observing $i$'s previous actions right after $\Omicron^t$}\\
 $i$ plays  	&&	&	 The event to which $j$ assigns probability one right after $\Omicron^t$\\
\hline
\hline
 In $\Omicron^t$		&&&					 \\
\hline
$\langle \textbf{all stay} \rangle$  &&&     $i$ is inactive and $I^{t+1}_{ji}=I^t_j$ \\
$\langle I^t_{i} \rangle$  &&&     $i$ is active and $I^{t+1}_{ji}=I^t_j\cap I^t_{i}$ \\
$\langle 1 \rangle$  &&& 	  $i$ is pivotal    \\
\hline
%\multicolumn{4}{c}{The belief of $j\in G_i$ after observing $i$'s previous actions right after $\Kappa^t_1$ {contingent} on $\Omicron^t$}\\
\\
 $i$ plays	&&			  & The event to which $j$ assigns probability one right after $\Kappa^t(1,1)$\\
\hline
\hline
	  In $\Omicron^t$	 	&		In $\Kappa^t(1,1)$	&		&		  \\
\hline
$\langle \textbf{all stay} \rangle$  & $\langle i \rangle$	&&    $i$ is inactive and $I^{t+1}_{ji}=I^t_j$  \\
$\langle I^t_{i} \rangle$  & $\langle \textbf{all stay} \rangle$	&&    $\#[R](\theta)< k$ \\
$\langle I^t_{i} \rangle$  & $\langle i \rangle$	&&   $(\#[R](\theta)\geq k )$ or  ($i$ is active and $I^{t+1}_{ji}=I^t_j\cap I^t_{i})$\\
$\langle 1 \rangle$  & $\langle \textbf{all stay} \rangle$	&&	  $\#[R](\theta)< k$    \\
$\langle 1 \rangle$  & $\langle i \rangle$	&&	  $\#[R](\theta)\geq k$  \\
\hline
%\multicolumn{4}{c}{The belief of $j\in G_i$ after observing $i$'s previous actions right after $\Kappa^t(2,\cdot)$ {contingent} on $\Omicron^t$, $\Kappa^t(1,\cdot)$, and $\Kappa^t(2,\cdot)$}\\
\\
 $i$ plays  	&		&  	  &The event to which $j$ assigns probability one right after $\Kappa^t(2,1)$\\
\hline
\hline
In $\Omicron^t$			&	In $\Kappa^t(1,1)$			&			In $\Kappa^t(2,1)$		&   \\
\hline
$\langle \textbf{all stay} \rangle$  & $\langle i \rangle$	&  $\langle \textbf{all stay} \rangle$ &  $i$ is inactive and $I^{t+1}_{ji}=I^t_j$  \\
$\langle I^t_{i} \rangle$  & $\langle \textbf{all stay} \rangle$	&  $\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)< k$ \\
$\langle I^t_{i} \rangle$  & $\langle i \rangle$	&  $\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)\geq k$ \\
$\langle I^t_{i} \rangle$  & $\langle i \rangle$	&  $\langle i \rangle$ &  $i\in R^t$ and $I^{t+1}_{ji}=I^t_j\cap I^t_{i}$ \\
$\langle 1 \rangle$  & $\langle \textbf{stay} \rangle$	&	$\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)< k$    \\
$\langle 1 \rangle$  & $\langle i \rangle$	&	$\langle \textbf{all stay} \rangle$ &  $\#[R](\theta)\geq k$  \\
  \hline
\end{tabular}
\end{center}
\end{table}
%\end{landscape}


If $p$ is certain $\#[R](\theta)< k$, $p$ plays $\langle \textbf{all stay} \rangle$ in the first sub-block of the first division in the coordination phase. Consequently, all $p$'s neighbors know $\#[R](\theta)< k$ right after that since $p$ has played $\langle 1 \rangle$ in the just finished reporting phase to announce he is pivotal. $p$'s neighbors then play $\langle \textbf{all stay} \rangle$ continuously in each sub-block in the first division in the coordination phase, and therefore all Rebels know $\#[R](\theta)< k$ contagiously by observing $\langle \textbf{all stay} \rangle$ being played. All Rebels play \textbf{s} forever after the first division in the coordination phase, and $T^{\theta}$ arrives. 

On the other hand, suppose $p$ is certain $\#[R](\theta)\geq k$, $p$ plays $\langle p \rangle$ in each sub-block in the first division in the coordination phase. To reveal $\#[R](\theta)\geq k$, $p$ plays $\langle \textbf{all stay} \rangle$ in the first sub-block of the second division in the coordination phase. Notice that $\langle \textbf{all stay} \rangle$ is a costless sequence of actions. It might not seem intuitive at first sight, but playing $\langle \textbf{all stay} \rangle$ effectively prevents another free-rider problem. Suppose there are two pivotal Rebels, say $p$ and $p^{'}$, who have already known $\#[R](\theta)\geq k$ right after the just finished reporting phase. If initiation to disseminate knowledge about $\#[R](\theta)\geq k$ incurs negative payoff, $p$ or $p^{'}$ will have the incentive, again, to wait for the other one initiates it. Playing $\langle \textbf{all stay} \rangle$ in the first sub-block of the second division in the coordination phase proudly becomes the initiation sequence by its cheapness. By the same contagion argument as the above mentioned, all Rebels play \textbf{r} after the third division in the coordination phase, and $T^{\theta}$ arrives.

The remaining question is why a non-pivotal Rebel, say $i$, does not mimic a pivotal Rebel's behavior by playing $\langle 1 \rangle$ in the just finished reporting phase even though it might be undetectable. The reason is the following. If $i$ plays $\langle 1 \rangle$, $i$'s neighbor will think $i$ is pivotal. According to the equilibrium path, it implies that all players play either \textbf{r} or \textbf{s} forever after the third division in the coordination phase; therefore, the belief updating is also terminated. What follows is $i$ cannot learn the relevant information after the third division in the coordination phase. If $i$ does not deviate, $i$ will learn the relevant information eventually and choose the best response to get the maximum payoff at every $\theta$. He prefers not to deviate if $\delta\in(0,1)$ is high enough.\footnote{By Lemma~\ref{lemma:learning_on_the_path} in Appendix, the relevant information is learnt by every Rebel on the path eventually.} 


\subsection{Sketch of the proof of Theorem~\ref{thm_main_result}}

In the above I have listed Rebels' behavior in Table~\ref{Table_msg_RP_path}, Table~\ref{Table_cd0}, Table~\ref{Table_blf_cdt}, and their belief updating in Table~\ref{Table_blf_cd0}, Table~\ref{Table_blf_cdt}, as the blueprint for the constructed equilibrium path. I sketch the proof of Theorem~\ref{thm_main_result} as follows.

First, I use out-of-path belief to prevent players from making detectable deviations, such as deviating from playing the specified forms of sequences that are listed in Table~\ref{Table_msg_form} or Table~\ref{Table_msg_coordination}. This out-of-path belief serves as a grim trigger: the punisher will play \textbf{stay} forever if he has not yet learnt the relevant information. Despite a detectable deviation might be detected by only a few Rebels owing to the network monitoring, I argue that any detectable deviation made by a deviant before he learns the relevant information will strictly reduce the possibility of achieving the ex-post efficient outcome. For any undetectable deviation made by a deviant before he learns the relevant information, I prove that it will also strictly reduce the deviant's expected continuation payoff. Suppose this undetectable deviation is meant to report less Rebels in a reporting phase, it will diminish the possibility of achieving the ex-post outcome. Suppose this one introduces ``noises'' in information sharing process so that the deviant will never learn the relevant information, his expected continuation payoff will be strictly reduced, as argued in the last paragraph in Section~\ref{sec:cdt}. Since the stage-game payoff after $T^{\theta}$ is maximum for every $\theta$ by following the equilibrium path, a high enough $\delta\in(0,1)$ will deter Rebels from deviating. I then conclude the proof.

\section{Discussion}
\label{sec:varies}
%
%
In the above APEX equilibrium construction, players act as if acting a sequence. Nevertheless, the actual description of an APEX equilibrium should specify how they act period by period and how they update belief at every period. This description is tedious; it is left in the Appendix.

Instead of providing further details in equilibrium construction, I discuss the scenario when pay-off is a signal and why my constructed APEX equilibrium may fail in cyclic networks.



%\subsubsection{Variation: Rebels with different levels of efforts}
%
%We may also consider a model in which players contribute different levels of efforts to a collective action. Let the set of states of nature be $\hat{\Theta}=\Theta \times \Xi$, where $\Theta=\{Rebel,Inert\}^n$ and $\Xi=\{1,2,...,k\}^n$. Let $\hat{\theta}=(\theta,e)$ be a state of nature. After $\hat{\theta}$ is realized, a player $i$ will hold an endowment $e_i$, where $e_i\in \{1,2,...,k\}$. The payoff structure is modified as the following.
%\begin{enumerate}
%\item $u_{Rebel_i}(a_{Rebel_i},a_{-\theta_i})=b_i$ if $a_{Rebel_i}=\textbf{revolt}$ and $\sum_{j:a_{\theta_j}=\textbf{revolt}}e_j\geq k$
%\item $u_{Rebel_i}(a_{Rebel_i},a_{-\theta_i})=-e_i$ if $a_{Rebel_i}=\textbf{revolt}$ and $\sum_{j:a_{\theta_j}=\textbf{revolt}}e_j< k$
%\item $u_{Rebel_i}(a_{Rebel_i},a_{-\theta_i})=0$ if $a_{Rebel_i}=\textbf{stay}$
%\item $u_{Inert_i}(a_{Inert_i},a_{-\theta_i})=1$ if $a_{Inert_i}=\textbf{stay}$
%\end{enumerate}
%
%
%
%After $\hat{\theta}$ is realized, players repeatedly play the above game in a network $G$. To see that the strategies constructed in previous section is still an equilibrium, we can %transform $(G,\hat{\Theta})$ to $(G^{'},\hat{\Theta}^{'})$, where each player $i$ is attached with $\#e_i$ different players in $G^{'}$, and $\hat{\Theta}^{'}=\Theta\times \{1\}^{n}$. 




\subsection{Payoff as signals}
The hidden payoff assumption can be relaxed without changing the main result. One may consider a situation in which the stage payoff depends not only on players' joint efforts but also on a random shock, say the weather. To fix the idea, there is a public signal $y\in \{r,s\}$ generated according to the action profile. Let a Rebel's payoff function be $u_{R}(a_{R},y)$ such that $u_{R}(\textbf{stay},r)=u_{R}(\textbf{stay},s)=u_0$. $y$ is drawn from the distribution of 
\begin{eqnarray*}
p_{rr} &=& \mathrm {Pr}(y=r|\#\{j:a_j\textbf{revolt}\}\geq k) \\
p_{sr}=1-p_{rr} &=& \mathrm {Pr}(y=s|\#\{j:a_j\textbf{revolt}\}\geq k) \\
p_{ss} &=& \mathrm {Pr}(y=s|\#\{j:a_j\textbf{revolt}\}< k) \\
p_{rs}=1-p_{ss} &=& \mathrm {Pr}(y=r|\#\{j:a_j\textbf{revolt}\}< k) \\
\end{eqnarray*}
such that
\begin{equation*}
p_{rr}u_{R}(\textbf{revolt}, r)+p_{sr}u_{R}(\textbf{revolt}, s)>u_0>p_{rr}u_{R}(\textbf{revolt}, r)+p_{ss}u_{R}(\textbf{revolt}, s),
\end{equation*}
and
\begin{equation*}
0\leq p_{rs}\leq 1,0\leq p_{ss}\leq 1.
\end{equation*}

The APEX equilibrium constructed for Theorem~\ref{thm_main_result} is still a one in this scenario. Note that in that APEX equilibrium path, at most one \textbf{revolt} can occur at every period before some Rebel plays $\langle 1 \rangle$. This implies that the signal $y$ is completely uninformative before some Rebel plays $\langle 1 \rangle$. If a Rebel $i$ deviates to play $\langle 1 \rangle$ in $\Omicron^t$ at some $t$ in the hope gathering information from $y$, he will not learn the relevant information after $\Omicron^t$ since the terminal period will come right after $t$-block. He will, however, learn the relevant information and play the ex-post efficient outcome if he is on the path, and hence he will not deviate.

%If $p_{rr}$ or $p_{ss}=1$, an APEX equilibrium can be easily constructed: all Rebels play \textbf{revolt} in the first period and then keep playing \textbf{revolt} or \textbf{stay} contingent on the signals $y=r$ or $y=s$ respectively.

\subsection{Cyclic networks}
\label{sec:cyclic}


Scenarios in cyclic networks substantially differ from the acyclic counterpart. The free-rider problem could become intractable in cyclic networks. Let us consider the configuration in Figure~\ref{fig:cyclic_network}, and suppose $k=4$.

\begin{figure}[!h]

\begin{center}
\begin{tikzpicture}[scale=1]
    % Draw a 7,11 network
    % First we draw the vertices
    \foreach \pos/\name in {{(2,3)/R_1}, {(3,2)/R_2}, {(4.5,2)/R_3}, {(5.5,3)/R_4}, {(3.75,4)/I_5}}
        \node[vertex] (\name) at \pos {$\name$};
    
    
    % Connect vertices with edges 
    \foreach \source/ \dest in { R_1/R_2, R_2/R_3, R_3/R_4, R_4/I_5, I_5/R_1}
        \path[edge] (\source) -- (\dest) ;
        
\end{tikzpicture}
\end{center}
\caption{A configuration of the state and the network in which player 1, 2, 3, 4 are Rebels while player 5 is an Inert.}
\label{fig:cyclic_network}
\end{figure}

In Figure~\ref{fig:cyclic_network}, Rebels 2 and 3 are $\theta$-pivotal by definition. From the perspective of Rebel 2, the type of player 5 could be Inert. Therefore, Rebel 2 does not know that Rebel 1 is pivotal. Similarly, Rebel 2 does not know that Rebel 3 is pivotal, \textit{even though} player 3 is indeed $\theta$-pivotal. Therefore there is no common knowledge of the free-rider problem at the beginning of $1$-block. 

However, the common knowledge of engaging in a free-rider problem is restored when we cut the edge between player 4 and 5; Rebel 2 knows that he is the only $\theta$-pivotal Rebel.

I leave a conjecture in this paper and end this section.

\begin{conjecture}
For any $n$-person repeated $k$-Threshold game with parameter $ k < n$ played in any network,
if $\pi$ has full support on strong connectedness, then there exists a $\delta^{*}\in (0,1)$ such that an APEX equilibrium exists whenever $\delta>\delta^{*}$.
\end{conjecture}
%
%
%

\section{Conclusion}
\label{sec:con}

I model a coordination game and illustrate the learning processes generated by strategies in a sequential equilibrium to answer the question proposed in the beginning: what kind of networks can conduct coordination in a collective action with information barrier. In the equilibrium, players transmit the relevant information by encoding such information by their actions as time goes by. Since there might be an negative expected payoff in coding information, the potential free-rider problems might occur to impede the learning process. My result show that if the network is acyclic, players can always learn the underlying relevant information and conduct the coordination only by actions. In cyclic networks, however, what kinds of equilibrium strategies can lead to learning the relevant information still remains to be answered.


The construction of the communication protocol by actions exploits the assumption of the common knowledge of the network and the finite type space. Since the relevant information has been parametrized as a threshold in the stage game, players can acquire this information by jointly incrementally reporting their own private information period by period. The major punishment to deter deviation is then the joint shifting to play that same action as the stopping to update information. The threshold game thus seems a potential model in proofing that a communication protocol by actions not only leads a learning process but also constitutes an equilibrium to reveal the relevant information in finite time.

Existing literatures in political science and sociology have recognized the importance of social network in influencing individual's behavior in participating social movements ( \citep{Passy2003}\citep{McAdam2003}\citep{Siegel2009}). This paper views networks as routes for communication in which rational individuals initially have local information but they can influence nearby individuals by taking actions. Such influence may take long time to travel across individuals and the whole process incurs inefficient outcomes in many periods. A characterization in the speed of information transmission across a network is not answered here, although it is an important topic in investigating the most efficient way to let the information be spread. This question would remain for the future research.



\bibliographystyle{abbrvnat}	% (uses file "plain.bst")
\bibliography{bigref}		% expects file "myrefs.bib"


\appendix
\section{Appendix}
\subsection{The APEX equilibrium for Theorem~\ref{thm_main_result}}
\subsubsection{Equilibrium path}
\label{sec:equilibrium_path}
By definition of information hierarchy, 
\begin{eqnarray*}
I^t_i & = & \bigcup_{k_1\in G_i}\bigcup_{k_2\in G_{k_0}}...\bigcup_{k_{t}\in G_{k_{t-1}}}I^1_{k_{t}}\\
&= & \{j\in [R](\theta): \text{$\exists$ a path $(i,k_1...k_{l},j)$ s.t.~$0\leq l\leq t-1$ and $\theta_i=\theta_{k_1}=...=\theta_{k_l}=R$}\}
\end{eqnarray*}
Let us define several notions.
\begin{definition}[Extended tree by $I^t_i$]
\label{def:ext_tree}
\begin{eqnarray*}
X^t_i & := &  \{j\in N: \\
	& & \text{$\exists$ a path $(i,k_1...k_{l},k_{l+1})$ s.t.~$k_{l+1}=j$, $l\geq t-1$, $\{i,k_1,...,k_{t}\}\subset I^t_i$}\}\cup I^t_i
\end{eqnarray*}
\end{definition}
Namely, $X^t_i $ is the set of all possible Rebels in $G$ given information $I^t_i$. 

\begin{definition}[The sub-tree rooting in $j$ but excluding $i$]
\[TR_{ij}:= \{v\in N:\text{there is a path from $i$ to $v$ through $j$, $j\in G_i$}\}\cup\{i,j\}\]
\end{definition}


\begin{definition}[Extended vertices outside $I^t_i$ in $TR_{ij}$]
\[Y^t_{ij}:= TR_{ij}\cap(X^t_i\setminus I^t_i)\]
\end{definition}


\begin{definition}
\[D^t_{i}:= \{j\in G_i: Y^t_{ij}\neq \emptyset\}\]
\end{definition}




\begin{definition}[Finite register machine]
A finite register machine for $i$ consists of finite registers $\Sigma$. A register is a tuple \[(\Omega, \times_{G_i}A_R,f,\lambda),\] in which $\Omega$ are sets of events induced by $H_i$. $\times_{G_i}A_R$ is the sets of input. $f:\Omega\rightarrow A_{R}$ assigns an action to each event. $\lambda: \Omega\times \times_{G_i}A_R \rightarrow \Sigma$ is the transition function. There is a set of initial registers. 
\end{definition}

$i$'s register specifies $i$'s action according his information at a certain period but does not characterize $i$'s information transition. The register machine here is more like the \textit{switch function} instead of the finite automata. The information of $i$ up to period $s$ is $P_i(\theta)\times \{h^s_{G_i}\}\times H^s_{N\setminus G_i}$ characterized in Section~\ref{sec:model}. 

\begin{definition}[$m$-register in $t$-block]
A $m$-register in a (sub)block or a division is the register in the $m$-th period in that (sub)block or division.
\end{definition}

To shorten the notation, denote $m\dashv\Gamma$ as the $m$-register in the (sub)block or division $\Gamma$.

\begin{definition}[Terminal \textbf{r}]
The terminal \textbf{r} is a register such that the image of $f$ is $\{\textbf{revolt}\}$ and the image of $\lambda$ is a singleton containing itself. 
\end{definition}

\begin{definition}[Terminal \textbf{s}]
The terminal \textbf{s} is a register such that the image of $f$ is $\{\textbf{stay}\}$ and the image of $\lambda$ is a singleton containing itself. 
\end{definition}

The equilibrium will be represented as a {finite register machine}. Moreover, though players act as if acting a whole sequence, they in fact act period by period. For convenience, for any finite sequence of action $\langle \rangle$, denote $\langle \rangle_m$ as the $m$-th (counting from the beginning) component in $\langle \rangle$, and denote $\langle \rangle(m)$ as the prefix of $\langle \rangle$ with length $m$. Let us also shorten action \textbf{revolt} to be \textbf{r} and \textbf{stay} to be \textbf{s}. 

\paragraph{Initial registers}
%\noindent{\textbf{Initial registers}
The initial register for each Rebel is $1\dashv\Kappa^1(1,\cdot)$, which is defined in the next section.
\paragraph{Registers in $\Kappa^1$}
%\noindent{\textbf{Equilibrium path in $\Kappa^1$}}

\begin{landscape}
\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^1_{1}$ on the path}
\label{table:eqm_path_k01}
\begin{center}
\begin{tabular}{c c | c | c | c }
\multicolumn{5}{c}{$1\leq m \leq |\Kappa^1(1,\cdot)|-1$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$\# X^1_i<k$  	& 	 &$\langle \textbf{all stay} \rangle_m$ &	& terminal \textbf{s}\\
$i\notin R^1$, $\# X^1_i\geq k$, $I^1_i< k$  	&  &$\langle \textbf{all stay} \rangle_m$ & 	& $m+1\dashv\Kappa^1_{1}$\\
$i\in R^1$, $\# X^1_i\geq k$, $I^1_i< k$  	& 	 &$\langle i \rangle_m$	&  & $m+1\dashv \Kappa^1_{1}$\\
$i\in R^1$, $\# X^1_i\geq k$, $I^1_i\geq k$  	& 	 &$\langle i \rangle_m$	&  & $m+1\dashv \Kappa^1_{1}$\\
\hline
\\
\multicolumn{5}{c}{$m=|\Kappa^1(1,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$\# X^1_i<k$  	& 	& $\langle \textbf{all stay} \rangle_m$	&     & terminal \textbf{s}\\
$i\notin R^1$, $\# X^1_i\geq k$, $I^1_i< k$   	& all $j$ play $\langle \textbf{all stay} \rangle(m-1)$ & $\langle \textbf{all stay} \rangle_m$	 & all $j$ play \textbf{s} & terminal \textbf{s}\\
$i\notin R^1$, $\# X^1_i\geq k$, $I^1_i< k$   	& $\exists j$ plays $\langle j \rangle(m-1)$ & $\langle \textbf{all stay} \rangle_m$	& such $j$ plays $\langle j \rangle_m$  & $1\dashv \Kappa^1(2,\cdot)$\\
$i\in R^1$, $\# X^1_i\geq k$, $I^1_i< k$   	& 	& $\langle i \rangle_m$	&& $1\dashv \Kappa^1(2,\cdot)$ \\
$i\in R^1$, $\# X^1_i\geq k$, $I^1_i\geq k$  	& 	& $\langle i \rangle_m$ &	& $1\dashv \Kappa^1(2,\cdot)$ \\
\hline
\end{tabular}
\end{center}
\end{table}



\end{landscape}

\begin{landscape}
\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^1_{2}$ on the path}
\label{table:eqm_path_k02}
\begin{center}
\begin{tabular}{c c | c | c | c}
\multicolumn{5}{c}{$1\leq m < |\Kappa^1(2,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$i\notin R^1$  	& & $\langle \textbf{all stay} \rangle_m$	&    & $m+1\dashv \Kappa^1_{2}$\\
$i\in R^1$, $I^1_i< k$  	& $\exists j\in G_i$, $j$ plays $<j>_j=s$	& $\langle \textbf{all stay} \rangle_m$	& 	& $m+1\dashv \Kappa^1_{2}$\\
$i\in R^1$, $I^1_i< k$  	& $\forall j\in G_i$, $j$ plays $<j>_j=r$ 	& $\langle i \rangle_m$	& 	& $m+1\dashv \Kappa^1_{2}$\\
$i\in R^1$, $I^1_i\geq k$  	& 	& $\langle \textbf{all stay} \rangle_m$	&	& $m+1\dashv \Kappa^1_{2}$\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^1(2,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$i\notin R^1$  	& $\forall j\in G_i$, $j$ plays $\langle j \rangle(m-1)$    & $\langle \textbf{all stay} \rangle_m$	& $\forall j\in G_i$, $j$ plays $\langle j \rangle_m$	& $1\dashv \Kappa^1_{3}$\\
$i\notin R^1$  	& $\exists j\in G_i$, $j$ plays $\langle \textbf{all stay} \rangle(m-1)$   & $\langle \textbf{all stay} \rangle_m$	& such $j$ plays $\langle \textbf{all stay} \rangle_m$	& terminal \textbf{r}\\
$i\in R^1$, $I^1_i< k$   	& $\forall j\in G_i$, $j$ play $\langle j \rangle(m-1)$ 	& $\langle i \rangle_m$	&  $\forall j\in G_i$, $j$ plays $\langle j \rangle_m$	& $1\dashv \Kappa^1_{3}$ \\
$i\in R^1$, $I^1_i< k$   	&  $\exists j\in G_i$, $j$ plays $\langle \textbf{all stay} \rangle(m-1)$ 	& $\langle i \rangle_m$	& such $j$ plays $\langle \textbf{all stay} \rangle_m$	&  terminal \textbf{r}\\
$i\in R^t$, $I^1_i\geq k$  	& 	& $\langle \textbf{all stay} \rangle_m$	& 	& terminal \textbf{r} \\
\hline
\end{tabular}
\end{center}
\end{table}

\end{landscape}





\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^1_{3}$ on the path}
\label{table:eqm_path_k03}
\begin{center}
\begin{tabular}{c c | c | c | c}
\multicolumn{5}{c}{$1\leq m < |\Kappa^1(3,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
  	&	& \textbf{s} & $\forall j$ play $\textbf{s}$ 	& $m+1\dashv \Kappa^1_{3}$\\
  	&  & \textbf{s}  &  $\exists j$ play $\textbf{r}$  	& terminal \textbf{r}\\
\hline
\\
\multicolumn{5}{c}{$1\leq m = |\Kappa^1(3,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
  	& 	& \textbf{s} & $\forall j$ play $\textbf{s}$ 	& $1\dashv \Omicron^1$\\
  	&  & \textbf{s}  &  $\exists j$ play $\textbf{r}$  	& terminal \textbf{r}\\
\hline
\end{tabular}
\end{center}
\end{table}



\clearpage

\paragraph{Registers in $\Omicron^t$}
%\noindent \textbf{Equilibrium path in $\Omicron^t$}
Let $m_i=|\Omicron^t|-x_{I^t_i}$ be the period in which $i$ report $I^t_i$. I.e.~$m_i$ is the period where \textbf{r} occurs in $\langle I^t_i \rangle$. 
Denote $G_i(m)=\{j\in G_i: m_j<m\}$. Define $I^{t+1}_i(m):= I^t_i\cup\bigcup_{j\in G_i(m)}I^t_j$ to be the information of $i$ up to the $m$-th period in $\Omicron^t$. 
%Similarly, define $i$'s extended neighbors to be $G^{t+1}_i(m):= G^t_i\cup(\bigcup_{j\in G_i, m_j<m}I^t_j)$. 
Define $X^{t+1}_i(m)$ to be the extended tree from $I^t_i(m)$ in the same way as that in Definition~\ref{def:ext_tree}, and define $Y^t_{ij}(m)$ and $D^t_i(m)$ accordingly.


\begin{landscape}
\begin{table}[!htbp]
\caption{The $m\dashv\Omicron^t$ on the path, where $1\leq m < |\Omicron^t|$}
\label{table:eqm_path_ot1}
\begin{center}
\begin{tabular}{c c | c | c | c}
%\multicolumn{5}{c}{$1\leq m < |\Omicron^t|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$i\notin R^t$  	& 								& $\langle \textbf{all stay} \rangle_m$		&  			& $m+1\dashv \Omicron^t$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal		 	&  $I^{t+1}_i(m)< k$, $X^{t+1}_i(m)<k$			&  $\langle \textbf{all stay} \rangle_m$	& 	& terminal \textbf{s} \\
$i\in R^t$, not free rider, not $k-1$-pivotal	  	& $I^{t+1}_i(m)<k$, $X^{t+1}_i(m)\geq k$		    & $\langle I^t_i \rangle_m$ 		&    			& $m+1\dashv \Omicron^t$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k$, $X^{t+1}_i(m)\geq k$	& $\langle 1 \rangle_m$ 	& 	& $m+1\dashv \Omicron^t$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k-1$, $X^{t+1}_i(m)\geq k$, $D^t_i=1$	& $\langle 1 \rangle_m$ 	& 	& $m+1\dashv \Omicron^t$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k-1$, $X^{t+1}_i(m)\geq k$, $D^t_i>1$	& $\langle I^t_i \rangle_m$ 	& 	& $m+1\dashv \Omicron^t$ \\
$i\in R^t$, the free rider  	&  $X^{t+1}_i(m)\geq k$ & $\langle 1 \rangle_m$ 		& 				  & $m+1\dashv \Omicron^t$ \\
$i\in R^t$, the free rider  	&  		$X^{t+1}_i(m)<k$					&  $\langle \textbf{all stay} \rangle_m$		& 										  & terminal \textbf{s} \\
$i\in R^t$, $i$ is $k-1$-pivotal  	&  $X^{t+1}_i(m)\geq k$ & $\langle 1 \rangle_m$ 	& 											 & $m+1\dashv \Omicron^t$ \\
$i\in R^t$, $i$ is $k-1$-pivotal  	&  	$X^{t+1}_i(m)<k$		&  $\langle \textbf{all stay} \rangle_m$	& 											 & terminal \textbf{s} \\
\hline
\end{tabular}
\end{center}
\end{table}


\end{landscape}



\begin{landscape}
\begin{table}[!htbp]
\caption{The $m\dashv\Omicron^t$ on the path, where $m=|\Omicron^t|$}
\label{table:eqm_path_ot2}
\begin{center}
\begin{tabular}{c c | c | c | c}
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
%$X^{t+1}_i(m)<k$  	&                                    & 											&				 				& to terminal \textbf{s}\\
$i\notin R^t$  	& 								& $\langle \textbf{all stay} \rangle_m$		&  			& $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal		 	&  $I^{t+1}_i(m)< k$, $X^{t+1}_i(m)<k$			&  $\langle \textbf{all stay} \rangle_m$	& 	& terminal \textbf{s} \\
$i\in R^t$, not free rider, not $k-1$-pivotal	  	& $I^{t+1}_i(m)<k-1$, $X^{t+1}_i(m)\geq k$		    & $\langle I^t_i \rangle_m$ 		&    			& $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k$, $X^{t+1}_i(m)\geq k$	& $\langle 1 \rangle_m$ 	& 	& $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k-1$, $X^{t+1}_i(m)\geq k$, $D^t_i=1$	& $\langle 1 \rangle_m$ 	& 	& $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, not free rider, not $k-1$-pivotal	 	&  $I^{t+1}_i(m)\geq k-1$, $X^{t+1}_i(m)\geq k$, $D^t_i>1$	& $\langle I^t_i \rangle_m$ 	& 	& $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, the free rider  	&  $X^{t+1}_i(m)\geq k$ & $\langle 1 \rangle_m$ 		& 				  & $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, the free rider  	&  		$X^{t+1}_i(m)<k$					&  $\langle \textbf{all stay} \rangle_m$		& 										  & terminal \textbf{s} \\
$i\in R^t$, $i$ is $k-1$-pivotal  	&  $X^{t+1}_i(m)\geq k$ & $\langle 1 \rangle_m$ 	& 											 & $1\dashv \Kappa^t_{1,1}$ \\
$i\in R^t$, $i$ is $k-1$-pivotal  	&  	$X^{t+1}_i(m)<k$		&  $\langle \textbf{all stay} \rangle_m$	& 											 & terminal \textbf{s} \\
\hline
\end{tabular}
\end{center}
\end{table}


\end{landscape}




\clearpage

\paragraph{Registers in $\Kappa^t$ for $t\geq 2$}
%\noindent \textbf{Equilibrium path in $\Kappa^t$ for $t\geq 1$}
\clearpage
\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^t(1,v)$ for $v=1,...,n$ on the path}
\label{table:eqm_path_kt1}
\begin{center}
\begin{tabular}{c c | c | c | c}
\multicolumn{5}{c}{$1\leq m < |\Kappa^t(1,v)|$, where $v=1,...,n$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$X^{t+1}_i<k$  	&                                & $\langle \textbf{all stay} \rangle_m$		&				 				& terminal \textbf{s}\\
$X^{t+1}_i\geq k$  	& 						& $\langle i \rangle_m$		&  $\exists j\in G_i, j=m\text{ such that } a_j=\textbf{s}$	& terminal \textbf{s}\\
$X^{t+1}_i\geq k$ 	& 						& $\langle i \rangle_m$		&  $\forall j\in G_i\text{ such that } a_j= \langle j \rangle_m$	& $m+1\dashv \Kappa^t(1,v)$\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^t(1,v)|$, where $v=1,...,n-1$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$X^{t+1}_i<k$  	&                                & $\langle \textbf{all stay} \rangle_m$	&				 				& terminal \textbf{s}\\
$X^{t+1}_i\geq k$ & 						& $\langle i \rangle_m$		&  $\exists j\in G_i, j=m\text{ such that } a_j=\textbf{s}$	& terminal \textbf{s}\\
$X^{t+1}_i\geq k$ 	& 						& $\langle i \rangle_m$		&  $\forall j\in G_i\text{ such that } a_j= \langle j \rangle_m$	& $1\dashv \Kappa^t_{1,v+1}$\\
\hline
\\
\multicolumn{5}{c}{$1\leq m < |\Kappa^t(1,n)|$}\\
\hline
\hline
$X^{t+1}_i\geq k$ 	& 						& $\langle i \rangle_m$		&  $\exists j\in G_i, j=m\text{ such that } a_j=\textbf{s}$	& terminal \textbf{s}\\
$X^{t+1}_i\geq k$ & 						& $\langle i \rangle_m$		&  $\forall j\in G_i\text{ such that } a_j= \langle j \rangle_m$	& $m+1\dashv \Kappa^t(1,n)$\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^t(1,n)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$X^{t+1}_i\geq k$ 	& 						& $\langle i \rangle_m$		&  $\exists j\in G_i, j=m\text{ such that } a_j=\textbf{s}$	& terminal \textbf{s}\\
$X^{t+1}_i\geq k$ 	& 						& $\langle i \rangle_m$		&  $\forall j\in G_i\text{ such that } a_j= \langle j \rangle_m$	& $1\dashv \Kappa^t_{2,1}$\\
\hline
\end{tabular}
\end{center}
\end{table}


\clearpage



\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^t(2,v)$ for $v=1,...,t+1$ on the path}
\label{table:eqm_path_kt2}
\begin{center}
\begin{tabular}{c l | c | c | c}
\multicolumn{5}{c}{$1\leq m < |\Kappa^t(2,v)|$, where $v=1,...,t+1$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$I^{t+1}_i< k$  	& 	$\exists j$, $\langle j \rangle_j=\textbf{s}$	& $\langle \textbf{all stay} \rangle_m$		&  	& $m+1\dashv \Kappa^t(2,v)$\\
$I^{t+1}_i< k$  	& 	$\forall j$, $\langle j \rangle_j=\textbf{r}$	& $\langle i \rangle_m$		&  	& $m+1\dashv \Kappa^t(2,v)$\\
$I^{t+1}_i\geq k$	 & 				& $\langle \textbf{all stay} \rangle_m$ 	& 		& $m+1\dashv \Kappa^t(2,v)$\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^t(2,v)|$, where $v=1,....,t$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$I^{t+1}_i< k$  	& 	$\exists j\in G_i$, $\langle j \rangle_j=\textbf{s}$	& $\langle \textbf{all stay} \rangle_m$		&  	& $1\dashv \Kappa^t_{2,v+1}$\\
$I^{t+1}_i< k$  	& 	$\forall j\in G_i$, $\langle j \rangle_j=\textbf{r}$	& $\langle i \rangle_m$		&  	& $1\dashv \Kappa^t_{2,v+1}$\\
$I^{t+1}_i\geq k$	 & 				& $\langle \textbf{all stay} \rangle_m$ 	& 		& $1\dashv \Kappa^t_{2,v+1}$\\
\hline
\\
\multicolumn{5}{c}{$1\leq m < |\Kappa^t(2,t+1)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$I^{t+1}_i< k$  	& 	$\exists j\in G_i$, $\langle j \rangle_j=\textbf{s}$	& $\langle \textbf{all stay} \rangle_m$		&  	& $m+1\dashv \Kappa^t(2,t+1)$\\
$I^{t+1}_i< k$  	& 	$\forall j\in G_i$, $\langle j \rangle_j=\textbf{r}$	& $\langle i \rangle_m$		&  	& $m+1\dashv \Kappa^t(2,t+1)$\\
$I^{t+1}_i\geq k$	 & 				& $\langle \textbf{all stay} \rangle_m$ 	& 		& $m+1\dashv \Kappa^t(2,t+1)$\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^t(2,t+1)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
$I^{t+1}_i< k$  	& 	$\exists j\in G_i$, $\langle j \rangle_j=\textbf{s}$	& $\langle \textbf{all stay} \rangle_m$		&  	& terminal \textbf{r}\\
$I^{t+1}_i< k$  	& 	$\forall j\in G_i$, $\langle j \rangle_j=\textbf{r}$	& $\langle i \rangle_m$		&  	& $1\dashv \Kappa^t_{3,1}$\\
$I^{t+1}_i\geq k$	 & 				& $\langle \textbf{all stay} \rangle_m$ 	& 		& terminal \textbf{r}\\
\hline
\end{tabular}
\end{center}
\end{table}




\clearpage









\begin{table}[!htbp]
\caption{The $m\dashv\Kappa^t(3,\cdot)$ on the path}
\label{table:eqm_path_kt3}
\begin{center}
\begin{tabular}{c c | c | c | c}
\multicolumn{5}{c}{$1\leq m < |\Kappa^t(3,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
  	&	& \textbf{s} & $\forall j\in G_i$, $j$ plays $\textbf{s}$ 	& $m+1\dashv \Kappa^t(3,\cdot)$\\
  	&  & \textbf{s}  &  $\exists j\in G_i$, $j$ plays $\textbf{r}$  	& terminal \textbf{r}\\
\hline
\\
\multicolumn{5}{c}{$m= |\Kappa^t(3,\cdot)|$}\\
$\omega_i$ 	 & 	   &	$f(\omega_i)$  &	$a_{G_i}$ & $\lambda(\omega_i,a_{G_i})$ \\
\hline
\hline
 	& 	& \textbf{s} & $\forall j\in G_i$, $j$ plays $\textbf{s}$ 	& $1\dashv \Omicron^{t+1}$\\
  	&  & \textbf{s}  &  $\exists j\in G_i$, $j$ play $\textbf{r}$  	& terminal \textbf{r}\\
\hline
\end{tabular}
\end{center}
\end{table}




\clearpage
\subsection{Missing proofs}
%\label{appx_network}
\noindent\textbf{Proof of Lemma~\ref{lemma_learn}}
%\begin{lemma*}[Learning in the APEX equilibrum]
%If the assessment $(\tau^*,\mu^{*})$ is an APEX equilibrium, then for all $\theta\in \Theta$, there is a finite time $T^{\theta}_i$ for every Rebel $i$ such that $\sum_{\theta\in\{\theta:[R](\theta)\geq k\}}\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})=$ either $1$ or $0$
%whenever $s\geq T^{\theta}_i$.
%\end{lemma*}
\begin{proof}
The proof is done by contraposition. Suppose Rebels' strategies constitute an APEX equilibrium. By definition of the APEX equilibrium, at every $\theta$, there is a period $T^{\theta}$ when all Rebels' actions start to repeat themselves. Let $T=\max_{\theta\in \Theta}{T^{\theta}}$. For Rebel $i$, let $T_i=T+1$, and suppose $0<\sum_{\theta:\#[R](\theta)\geq k}\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})<1$ for some $s\geq T_i$. Then this Rebel assigns positive weight at some ${\theta^{'}}\in \{\theta:\#[R](\theta)< k\}$ and some positive weight at some ${\theta^{''}}\in \{\theta:\#[R](\theta)\geq k\}$ at period $s$. Note that $i$ has already known $\theta_j$ if $j\in G_i$, and therefore $i$ assigns positive weight at some $\theta^{'}\in \{\theta:\#[R](\theta)< k, \theta_l=R, l\notin G_i\}$ and positive weight at some $\theta^{''}\in \{\theta:\#[R](\theta)< k, \theta_l=I, l\notin G_i\}$. Since all Rebels' actions start to repeat themselves at period $T$, $i$ cannot update information afterwards. Suppose $i$'s continuation strategy is to continuously play \textbf{revolt}, then this is not ex-post efficient when $\#[R](\theta)< k$; suppose $i$'s continuation strategy is to continuously play \textbf{stay}, then this is not ex-post efficient when $\#[R](\theta)\geq k$.
\end{proof}
%
%
\bigskip
\noindent\textbf{Proof of Theorem~\ref{thm_minor_thm}}
%\begin{theorem*}[APEX equilibrium for the case of $k=n$]
%For any $n$-person repeated $k$-Threshold game with parameter $k=n$ played in a network, there is a $\delta^{*}$ such that a sequential APEX equilibrium exists whenever $\delta>
%\delta^{*}$.
%\end{theorem*}
\begin{proof}
Let $\tau^{*}$ be the following strategy. After the nature moves, a Rebel $i$ plays \textbf{revolt} if he has no Inert neighbor; $i$ plays \textbf{stay} forever if he has an Inert neighbor. After the first period, if $i$ has not detected a deviation and observes that all his Rebel neighbors play \textbf{revolt} continuously previously, he plays \textbf{revolt} in the current period; otherwise, he plays \textbf{stay} afterwards and forever. If a Rebel $j$ deviates, then $j$ plays \textbf{stay} afterwards and forever.

At period $s$, according to $\tau^{*}$, if $i$ has not detected a deviation, but he observe his Rebel neighbors plays \textbf{stay} in the current period, he forms the belief of \[\sum_{\theta:\#[R](\theta)\geq k}\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})=0\] afterwards and forever. Therefore, he plays \textbf{stay} afterwards and forever as his best response. 

At period $s$, if a Rebel detects a deviation, or he has deviated, to play \textbf{stay} afterwards and forever is his best response since at least one player will play \textbf{stay} afterwards and forever. 

Since the network is finite with $n$ vertices, if all players do not deviate, after period $n$, each Rebel plays \textbf{revolt} and gets payoff $1$ forever if $\theta\in \{\theta: \#[R](\theta)\geq k\}$; each Rebels plays \textbf{stay} and gets payoff $0$ forever if $\theta\in \{\theta: \#[R](\theta)< k\}$. However, a Rebel who has deviated surely gets payoff $0$ forever after period $n$. Therefore, there is a $0<\delta<1$ large enough to impede Rebels to deviate.

To check if $\tau^{*}$ and $\{\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})\}_{i\in N}$ satisfy full consistency\footnote{Krep and Wilson (1982)}, take any $0<x<1$ such that Rebels play $\tau^{*}$ with probability $1-x$ and play other behavior strategies with probability $x$. Clearly, when $x \rightarrow 0$, the belief converges to $\{\beta^{\pi,\tau^*}_{G_i}(\theta|h^{s}_{G_i})\}_{i\in N}$. Since the out-of-path strategy is the best response for both of the Rebel who detects deviation and the Rebel who makes deviation, for arbitrary beliefs they hold, $\tau^{*}$ is a sequential equilibrium.
\end{proof}
%
%
%\bigskip
%\noindent\textbf{Proof of Lemma~\ref{lemma_inclusion}}
%%\begin{lemma*}
%%%\label{lemma_inclusion}
%%If the $\theta$ has strong connectedness, then 
%%\[R^t\subseteq R^{t-1}\] for all $t\geq 1$
%%\end{lemma*}
%\begin{proof}
%I show that if $i\notin R^{t-1}$ then $i\notin R^t$ for all $t$. 
%
%By definition, 
%\begin{eqnarray*}
%G^t_i & = & \bigcup_{k_1\in G_i}\bigcup_{k_2\in G_{k_0}}...\bigcup_{k_{t}\in G_{k_{t-1}}}G^{1}_{k_{t}}\\
%&= & \{j\in N: \text{$\exists$ a path $(i,k_1...k_{l},j)$ such that $l\leq t-1$ and $\theta_i=\theta_{k_1}=...=\theta_{k_l}=R$}\},
%\end{eqnarray*}
%while
%\begin{eqnarray*}
%I^t_i & = & \bigcup_{k_1\in G_i}\bigcup_{k_2\in G_{k_0}}...\bigcup_{k_{t}\in G_{k_{t-1}}}I^{1}_{k_t}\\
%&= & \{j\in [R](\theta): \text{$\exists$ a path $(i,k_1...k_{l},j)$ such that $l\leq t-1$ and $\theta_i=\theta_{k_1}=...=\theta_{k_l}=R$}\}.
%\end{eqnarray*}
%The above equality says that, at $t=\dot{t}$, if $i\notin R^{\dot{t}}$, then there is a $j$ such that the Rebels, who can be reached by $\dot{t}$ consecutive edges from $i$, can be also reached by $\dot{t}$ consecutive edges from $j$. Therefore, if there are new Rebels who can be reached from $i$ at any $\ddot{t}>\dot{t}$ by $\ddot{t}$ consecutive edges, those new ones can be also be reached by $\ddot{t}$ consecutive edges by $j$. Hence, $i\notin R^{\ddot{t}}$. 
%\end{proof}



%
\bigskip
\noindent\textbf{Proof of Theorem~\ref{lemma_empty}}
%\begin{lemma*}
%If the network is acyclic and if the $\theta$ has strong connectedness, then 
%\[[R](\theta)\neq \emptyset \Rightarrow \text{ there exists } t\geq 0 \text{ and } i\in R^t \text{ such that }I^{t+1}_i=[R](\theta).\] 
%\end{lemma*}
\begin{proof}

%\begin{definition}[A tree rooted in $i$ and spanning in the direction toward $j$]
%\[TR_{ij}:= \{v\in N:\text{there is a path from $i$ to $v$ through $j$}\}.\]
%\end{definition}


Since the network is finite, $\theta$ has strong connectedness, and $[R](\theta)\neq \emptyset$, there is a minimum $t_i$ such that $I^{t_i}_i=[R](\theta)$ for each $i$ by the definition of $I^t_i$. Let $P=\underset{i\in N}{\arg\min}\{t_1,...,t_n\}$ with generic element $p$. Therefore $I^{t_p}_p=[R](\theta)$. I show that $p\in R^{t_p-1}$ to complete the proof. I prove it by contradiction. If $p\notin R^{t_p-1}$, then $I^{t_p-1}_p\subseteq G^{t_p-1}_j$ for some $j\in G_p$. Then, all the Rebels in $TR_{jp}$ are in $G^{t_p-1}_j$, but there exist Rebels in $TR_{pj}$ who are in $G^{t_p-1}_j$ but not in $I^{t_p-1}_p$. This is because the network is acyclic and $I^{t_p-1}_p\subset [R](\theta)$. But then $p\notin P$ since $I^{t_j-1}_j=[R](\theta)$ already. I then conclude that $p\in R^{t_p-1}$.

\end{proof}





\noindent\textbf{Proof of Lemma~\ref{lemma_at_most_two_nodes}}
%\begin{lemma*}
%If the network is acyclic and if $\pi$ has full support on strong connectedness, then for each $t$-block, there are at most two $\theta$-pivotal Rebels. Moreover, if there are two of them, they are neighbors.\footnote{As a remark, the above lemma is not true when the network is cyclic. To see this, consider a 4-player circle when $\theta=(R,R,R,R)$.}
%\end{lemma*}
   
\begin{proof}
The proof is by contradiction. Suppose that, at $t$-block and before $T^{\theta}$, there are three or more $\theta$-pivotal Rebels. Since $\theta$ has strong connectedness, there are three of them, $p_1,p_2,p_3$, with the property $p_1\in G_{p_2}$ and $p_2 \in G_{p_3}$. 

Since the network is acyclic, $p_1\notin G_{p_3}$ and $p_3\notin G_{p_1}$. Since $p_1$ is $\theta$-pivotal, $I^t\subset [R](\theta)$ and $I^{t+1}_p=[R](\theta)$. It implies that, in $TR_{p_1p_2}$, $p_1$ can reach all Rebels by $t+1$ edges, but cannot reach all of them by $t$ edges. The same situation applies to $p_3$. However, it means that $p_2$ can reach all Rebels in $TR_{p_1p_1}$ by $t$ edges and reach all Rebels in $TR_{p_1p_1}$ by $t$ edges, and hence $I^t_{p_2}=[R](\theta)$. It contradict to the definition of $\theta$-pivotal Rebel.

\end{proof}



\bigskip
\noindent\textbf{Proof of Lemma~\ref{lemman_pivotals_CK}}
%\begin{lemma*}
%If the network is acyclic and if $\pi$ has full support on strong connectedness, then for each $t$-block, if there are two $\theta$-pivotal Rebels $p$ and $p^{'}$, then they commonly know that they are $\theta$-pivotal Rebels at the beginning of $t$-block.
%\end{lemma*}
\begin{proof}
A $\theta$-pivotal $p$ knows that $p^{'}\in G_i$ if $p^{'}$ is another one. $p$ picks a neighbor $p'$ and checks whether or not $[R](\theta)\subseteq I^t_{p}\cup I^t_{p^{'}}$ for all possible $I^t_{p^{'}}$. By common knowledge of the network, $p$ knows $G^t_{p{'}}$. Since $p$ is $\theta$-pivotal, he is certain that all the Rebel in the direction from $p$ toward $p^{'}$ is in $G^t_{p^{'}}$ and hence in $I^t_{p^{'}}$. Then $p$ can check whether or not $[R](\theta)\subseteq I^t_{p}\cup I^t_{p^{'}}$ for all possible $I^t_{p^{'}}$. If so, then $p$ knows $p^{'}$ is also $\theta$-pivotal by the definition of $\theta$-pivotal. Similarly, a $\theta$-pivotal $p^{'}$ can do the same procedure. Therefore, if there are two $\theta$-pivotal $p$ and $p^{'}$, they commonly know that they are $\theta$-pivotal. They commonly know this at the beginning of $t$-block since they know $I^t_{p}$ and $I^t_{p^{'}}$ by the construction of information hierarchy.
\end{proof}






\bigskip
\noindent\textbf{Proof of Theorem~\ref{thm_main_result}}
I begin with the following lemmas stating that all Rebels eventually learn the relevant information on the path.
\begin{lemma}
\label{lemma:learning_on_the_path}
If the network is acyclic and if the $\theta$ has strong connectedness, then the equilibrium path specified in Section~\ref{sec:equilibrium_path} is an APEX strategy.
\end{lemma}
\begin{proof}
Firstly, suppose $\theta$ satisfies $\#[R](\theta)<k$. I show that, all Rebels will enter terminal \textbf{s} eventually without entering terminal \textbf{r}.  Let $p$ be the Rebel defined in the proof of Theorem~\ref{lemma_empty} so that $I^{t_p}_p=[R](\theta)$, where $p$ is one of the earliest Rebels who knows $\#[R](\theta)<k$. I claim that $\#X^{t_p}_p< k$ if and only if $I^{t_p}_p=[R](\theta)$. For the only if part, the proof is by way of contradiction. If not, by the full support on strong connectedness, there is a possible Rebel outside $I^{t_p}_p$ , and therefore $p$ is uncertain $\#[R](\theta)<k$. For the if part, note that $I^{t_p}_p\subset X^{t_p}_p$ and therefore $\#I^{t_p}_p< \#X^{t_p}_p<k$. I also claim that $\#X^{t_p}_p(m)<k$ if and only $I^{t_p}_p(m)=[R](\theta)$. The proof is exactly the same as the noted above by replacing $I^{t_p}_p$ to $I^{t_p}_p$ and $I^{t_p}_p(m)$ to $I^{t_p}_P(m)$

Referring to Table~\ref{table:eqm_path_k01} to Table~\ref{table:eqm_path_ot2}, whenever there is a $p$ so that $\#X^{t_p}_p<k$, $p$ plays \textbf{stay} forever. It implies that all Rebels enter terminal \textbf{s} right after $\Kappa^t_1, t\geq 0$. Notice that Rebels entering to terminal \textbf{r} only after some period after $\Kappa^t_1$ and therefore all Rebels will enter terminal \textbf{s} before terminal \textbf{r}. 

Secondly, suppose $\theta$ satisfies $\#[R](\theta)\geq k$. I show that all Rebels will enter terminal \textbf{r} eventually. Note first that if there is a Rebel $p$ so that $\#I^1_p\geq k$, all Rebels enter terminal \textbf{r} after $\Kappa^1(3,\cdot)$ by referring to Table~\ref{table:eqm_path_k01}, Table~\ref{table:eqm_path_k02}, and Table~\ref{table:eqm_path_k03}. At $t>0$, if there is a Rebel $p$ who has play $\langle 1 \rangle$ at $\Omicron^t$, by the postulate of $\#[R](\theta)\geq k$, after $\Kappa^t(3,\cdot)$, all Rebels enter terminal \textbf{r} according to the equilibrium path specified in Table~\ref{table:eqm_path_ot1}, Table~\ref{table:eqm_path_ot2}, Table~\ref{table:eqm_path_kt1}, Table~\ref{table:eqm_path_kt2}, and Table~\ref{table:eqm_path_kt3}. There must be some Rebel $p\in R^t$ who plays $\langle 1 \rangle$ at $\Omicron^t$ for some $t$ by the same argument in the proof of Theorem~\ref{lemma_empty}.
%a $\theta$-pivotal or whose $I^{t+1}_p(m)\geq k-1, D^t_p(m)=1$, then $p$ plays $\langle 1 \rangle$. If $p$ is a free rider or is a single $\theta$-pivotal, his neighbor will report truthful information or $\langle 1 \rangle$ to him. Thus he will know $\#[R](\theta)\geq k$ after $\Omicron^t$. If $p$ has $I^{t+1}_p(m)\geq k-1$ but $\#X^{t+1}_p(m)\geq k$, there are two cases. The first case is there is Rebel neighbor $j$ will report some $I^t_j$ such that $\# I^t_j\setminus I^{t+1}_p(m)\geq 1$ by the postulate of $\#[R](\theta)\geq k$, $\#X^{t+1}_p(m)\geq k$, and the full support on strong connectedness. The second case is there is Rebel neighbor $j$ who plays $\langle 1 \rangle$. The first case implies $\#I^{t+1}_p\geq k$. The second case implies $\#I^{t+1}(m)_p\geq k-1$ and $I^{t+1}(m)_p\geq k-1$, and hence $I^{t+1}_i\geq k$ if the network is acyclic. 
%In either case, after $\Kappa^t(3,\cdot)$, all Rebels enter terminal \textbf{r} according to the equilibrium path specified in Table~\ref{table:eqm_path_ot1}, Table~\ref{table:eqm_path_ot2}, Table~\ref{table:eqm_path_kt1}, Table~\ref{table:eqm_path_kt2}, and Table~\ref{table:eqm_path_kt3}. 
\end{proof}


Due to Lemma~\ref{lemma:learning_on_the_path}, define $T^{\theta}_{\tau^{*}}$ as the earliest period at which all Rebels play ex-post efficient outcome afterwards according to an APEX equilibrium $\tau^{*}$.  For simplicity, I suppress the notation $\beta^{\pi,\tau}_{G_i}(\theta|h^s_{G_i})$ to $\beta^{\tau}_{G_i}(\theta|h^s_{G_i})$ and the notation $\alpha^{\pi,\tau}_{G_i}(\theta, h^{s}|\theta_{G_i},h^{s}_{G_i})$ to $\alpha^{\tau}_{G_i}(\theta, h^{s}|h^{s}_{G_i})$. If $P(\theta)$ is a property of $\theta$, define \[\beta^{\tau}_{G_i}(P(\theta)|h^s_{G_i}):= \sum_{\theta\in\{\theta:P(\theta)\}}\beta^{\tau}_{G_i}(\theta|h^s_{G_i}).\] Furthermore, if $m,s$ are periods and $m>s$, denote $h^{m|s}$ as a history in $H^m$ so that $(h^s,h^{m|s})\in H^m$. Denote $\tau^{'}|^s_{\tau}$ as a strategy following $\tau$ til period $s$.

\begin{claim}
\label{claim:uncertain}
Suppose Rebel $i$ follows an APEX equilibrium $\tau^{*}$ til period $s$. If there is a strategy $\tau|^s_{\tau^{*}}=(\tau_i,\tau_{-i})|^s_{\tau^{*}}$  generating a history $h^{m|s}, \infty>m>s$ so that $i$ will be uncertain about the relevant information and stop belief updating after $m$, then Rebel $i$ will not deviate to $\tau|^s_{\tau^{*}}$ if $\delta\in(0,1)$ is sufficiently high.
\end{claim}

\begin{proof}
Denote $\beta^{\tau|^s_{\tau^{*}}}_{G_i}(\theta|h^{m|s}, h^s_{G_i})$ as $i$'s belief about $\theta$ at $m$ following $h^{m|s}$ induced by $\tau|^s_{\tau^{*}}$. By the postulate,  $0<\beta^{\tau|^s_{\tau^{*}}}_{G_i}(\#[R](\theta)|h^{m|s}, h^s_{G_i})<1$. From the perspective that $i$ holds a belief of $\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})$ at period $s$, $h^{m|s}$ can be thought of an imperfect signal at period $m$ to infer whether or not $\#[R](\theta)\geq k$: if $\#[R](\theta)\geq k$, $h^{m|s}$ occurs with probability $\eta$ and does not occur with probability $1-\eta$; if $\#[R](\theta)< k$, $h^{m|s}$ occurs with probability $\mu$ and does not occur with probability $1-\mu$ so that $0\leq \eta \leq 1$, $0\leq\mu\leq 1$, and $0<\eta/\mu<\infty$. Denote $M=\max\{m, T^{\theta}_{\tau^{*}}\}$. Rebel $i$'s maximum expected stage-game payoff starting from $M$ by following $h^{m|s}$ calculated at period $s$ is 
\[V=\max\{\eta\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})-\mu\beta^{\tau^{*}}_{G_i}(\#[R](\theta)< k|h^s_{G_i}),0\}.\]
The first term $\eta\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})-\mu\beta^{\tau^{*}}_{G_i}(\#[R](\theta)< k|h^s_{G_i})$ is $i$'s expected stage-game payoff if all Rebels play \textbf{revolt} afterwards starting from $M$.  The second term $0$ is the one by playing \textbf{stay} afterwards. Rebel $i$'s expected stage-game payoff starting from $M$ by following $\tau^{*}$ calculated at period $s$ is 
\[\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})>V.\]
The inequality above is due to $0<\eta<1,0<\mu<1$. There is a difference in present value of
\[W(\delta)=\frac{\delta^{M-s}(\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})-V)}{1-\delta}.\]
Denote $L$ as the summation of all gains from deviation calculated from period $s$ to period $M$. $L$ is finite since the stage-game payoff is finite and $M-s$ is finite. Taking sufficiently high $\delta\in(0,1)$ so that $W(\delta)>L$ will deter this deviation.
\end{proof}

\begin{claim} Suppose Rebel $i$ follows an APEX equilibrium $\tau^{*}$ til period $s$. If $i$ deviates to a strategy $\tau|^s_{\tau^{*}}=(\tau_i,\tau_{-i})|^s_{\tau^{*}}$ so that there are $d$ Rebels, $d>0$, detects this deviation, then Rebel $i$ will not deviate to $\tau|^s_{\tau^{*}}$ if $\delta\in(0,1)$ is sufficiently high.
\end{claim}

\begin{proof}
There are two cases: $I^s_i< k$ or $I^s_i\geq k$. If $s\in \Omicron^t$ for some $t\geq 1$, then $I^s_i$ refers to $I^s_i=I^{t+1}_i(m)$ such that $m$ is the $m$-th period in $\Omicron^t$ and $s=m+\sum^t_{\gamma=1}(|\Kappa^{\gamma}|+|\Omicron^{\gamma-1}|)$, where $|\Omicron^{0}|=0$. 

Suppose $I^s_i< k$. If $\tau|^s_{\tau^{*}}$ leads to a history so that $i$ will never learn the relevant information, $i$ will not deviate, by Claim~\ref{claim:uncertain}. If $\tau|^s_{\tau^{*}}$ leads to a history so that $i$ is certain that there are $k^{'}$ Rebels, $k\leq k^{'}<k+d$, play some action forever after some period $m_d$, $i$'s stage-game payoff  is at most 0 after $m_d$. If Rebel $i$ follows $\tau^{*}$, $i$'s stage-game payoff is 1 after $T^{\theta}$. Therefore after $M=\max\{T^{\theta}, m_d\}$, there is a difference as least $1/(1-\delta)$ as the present value at $M$. Since $I^s_i< k$, $0<\beta^{\tau^{*}}_{G_i}(\#[R](\theta)\geq k|h^s_{G_i})<1$, and therefore $\eta=\beta^{\tau^{*}}_{G_i}(k\leq \#[R](\theta)< k+d|h^s_{G_i})$ is positive. It implies there is a difference at least $\eta\delta^{M-s}/(1-\delta)$ as the present value at $s$. As for the case $\tau|^s_{\tau^{*}}$ leads to a history so that $i$ is certain that there are $k^{'}$ Rebels, $k^{'}\geq k+d$ or $k^{'}<k$, $i$'s stage-game payoff after $M=\max\{T^{\theta},m_d\}$ is as same as that if $i$ follows $\tau^{*}$. Denote $L$ as the summation of all gains from deviation calculated from period $s$ to period $M$. $L$ is finite since both stage-game payoff and $M-s$ are finite. Taking sufficiently high $\delta\in(0,1)$ so that $\eta\delta^{M-s}/(1-\delta)>L$ will deter this deviation.

Suppose $I^s_i\geq k$. Since $i$ follows $\tau^{*}$ til $s$, there are two cases: $s\in\Omicron^{t}$ for some $t\geq 1$ or $s\in \Kappa^t$ for some $t\geq 1$. Suppose $s\in\Omicron^{t}$, $i$ will be ready to play $\langle 1 \rangle$ in $\Omicron^{t}$. Because playing $\langle I \rangle$, where $I$ is a subset of $N$, incurs more negative payoff than $\langle 1 \rangle$ since there is discounting and there is at most one another player might play $\langle I \rangle$, $i$ will not deviate in this case.  Suppose $s\in\Kappa^{t}$. If $s\in \Kappa^t(1,v)$, where $v=1,...,n$, $i$'s detectable deviation is to play a sequence other than what Table~\ref{Table_msg_coordination} specifies, it will be detected by all $i$'s neighbors. There is a positive probability that all $i$'s neighbors will play \textbf{stay} forever according to the out-of-path belief and then all Rebels will contagiously play \textbf{stay} forever. $i$'s expected continuation payoff is 0 from some timing after $s$. The same situation happens if $s\in \Kappa^t(2,v)$, where $v=1,...,t+1$, or $s\in \Kappa^t(3,v)$, where $v=1,...,n$. Take $m_s$ as the period when all Rebels play \textbf{stay} forever. $m_s$ is finite since the network is finite. Then take $M=\max\{T^{\theta},m_s\}$. There is a difference at least $\eta\delta^{M-s}/(1-\delta)$ as the present value at $s$. Denote $L$ as the summation of all gains from deviation calculated from period $s$ to period $M$. $L$ is finite since both stage-game payoff and $M-s$ are finite. Taking sufficiently high $\delta\in(0,1)$ so that $\eta\delta^{M-s}/(1-\delta)>L$ will deter this deviation.
\end{proof}




\end{document}
